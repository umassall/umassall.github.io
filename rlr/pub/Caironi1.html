<HTML><HEAD>
<TITLE>Gradient-Based Reinforcement Learning: Learning Combinations of Control Policies</TITLE></HEAD>
<BODY bgcolor="FFFFBB">
<H2>Gradient-Based Reinforcement Learning: Learning Combinations of Control Policies</H2>
<!nextperson><B>Caironi, Pierguido</B><blockquote><A HREF= ftp://www.elet.polimi.it/pub/data/Pierguido.Caironi/tr97_50.ps.gz>Gradient-Based Reinforcement Learning: Learning Combinations of Control Policies</A><BR>

<I> Technical Report 97.50, Dip. Elettronica e Informazione, Politecnico di Milano </I>

( gzipped Postscript - 381 KB )

<BR><BR><B>Abstract</B>: This report presents two innovative model-based reinforcement
learning algorithms for continuous state-action environments:
GREMLIN-M and GREMLIN-MS.

The two algorithms learn optimal combinations of control
policies for autonomous agents.  GREMLIN-M learns an optimal
combination of fixed base control policies.  GREMLIN-MS
extends GREMLIN-M enabling the agent to learn simultaneously
the base control policies as well.

GREMLIN-M and GREMLIN-MS optimize a performance function
equal to the sum of the expected reinforcements in a sliding
temporal window of finite length.  The optimization is carried
out through gradient ascent with respect to the parameter
values of the control functions.

GREMLIN-M and GREMLIN-MS lend themselves to a motivational
interpretation.  That is, the combination function resulting
from learning may be seen as a representation of the motivations
to apply any single base control policy in different environmental
conditions.

</BLOCKQUOTE>
