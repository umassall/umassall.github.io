<HTML><HEAD>
<TITLE>Tree-based batch mode reinforcement learning</TITLE></HEAD>
<BODY bgcolor="FFFFBB">
<H2>Tree-based batch mode reinforcement learning</H2>
<!nextperson><B>Ernst, Damien</B> , Geurts Pierre, Louis Wehenkel<blockquote><A HREF= http://www.montefiore.ulg.ac.be/~ernst>Tree-based batch mode reinforcement learning</A><BR>

<I> Journal of Machine Learning Research, April 2005, Volume 6, pp 503-556 </I>

(Pdf - 1290 KB)

<BR><BR><B>Abstract</B>: Reinforcement learning aims to determine an optimal control policy from interaction with a system or from observations gathered from a system.  In batch mode, it can be achieved by approximating the so-called   Q-function based on a set of four-tuples (x_t,  u_t , r_t, x_{t+1})  where  x_t denotes the system state at time  t,  u_t the control action  taken, r_t the instantaneous reward obtained and x_{t+1} the successor  state of the system, and by determining the control policy from this  
Q-function. The Q-function approximation may be obtained from the limit  of a sequence of (batch mode) supervised learning problems. Within this  framework we describe the use of several classical tree-based  supervised learning methods (CART, Kd-tree, tree bagging) and two newly  proposed ensemble algorithms, namely extremely and totally randomized  trees. We study their performances on several examples and find that  the ensemble methods based on regression trees perform well in  extracting relevant information about the optimal control policy from  sets of four-tuples. In particular, the totally randomized trees give  good results while ensuring the convergence of the sequence, whereas by  relaxing the convergence constraint even better accuracy results are  provided by the extremely randomized trees.
</BLOCKQUOTE>
