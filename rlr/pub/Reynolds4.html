<HTML><HEAD>
<TITLE>Decision Boundary Partitioning: Variable Resolution Model-Free Reinforcement Learning</TITLE></HEAD>
<BODY bgcolor="FFFFBB">
<H2>Decision Boundary Partitioning: Variable Resolution Model-Free Reinforcement Learning</H2>
<!nextperson><B>Reynolds, Stuart</B><blockquote><A HREF= http://www.cs.bham.ac.uk/~sir/pub/ml2k_DBP.ps.gz>Decision Boundary Partitioning: Variable Resolution Model-Free Reinforcement Learning</A><BR>

<I> ICML-2k </I>

( gzipped Postscript - 241 KB )

<BR><BR><B>Abstract</B>: This paper presents a method to refine the resolution of a continuous state Q-function. Q-functions serve as an estimate of return for model-free reinforcement learning agents and are modified as a result of an agent's interaction with the environment. Traditional (non-adaptive) methods of approximating this function are bound by the parameters and resources with which they are initially provided. To overcome these limitations, the method presented here starts with a coarse discrete representation of the Q-function and refines those areas which are most important for the purposes of decision making. 
</BLOCKQUOTE>
