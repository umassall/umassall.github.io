<HTML><HEAD>
<TITLE>Reinforcement Learning with high-dimensional continuous actions	</TITLE></HEAD>
<BODY bgcolor="FFFFBB">
<H2>Reinforcement Learning with high-dimensional continuous actions	</H2>
<!nextperson><B>Baird, Leemon</B> , H. Klopf<blockquote><A HREF= http://www.cs.cmu.edu/~baird/papers/wirefit/index.html>Reinforcement Learning with high-dimensional continuous actions	</A><BR>

<I> Technical Report WL-TR-93-1147, Wright Laboratory, Wright-Patterson Air Force Base, 1993 </I>

(HTML - )

<BR><B>Abstract</B>: 
Many reinforcement learning systems, such as Q-learning (Watkins, 1989),
or
advantage updating (Baird, 1993), require that a function
<i>f</i>(<b>x</b>,<b>u</b>) be learned, and that the value of 
<IMG SRC="imgs/baird41.gif">
be calculated quickly for any given <b>x</b>.  The function <i>f</i> could
be
learned by a function approximation system such as a multilayer
perceptron, but
the maximum of <i>f</i> for a given <b>x</b> cannot be found analytically
and
is difficult to approximate numerically for high-dimensional <b>u</b>
vectors.
A new method is proposed, <i>wire fitting</i>, in which a function
approximation system is used to learn a set of functions called <i>control
wires</i>, and the function <i>f</i> is found by fitting a surface to the
control wires.  Wire fitting has the following four properties:  (1) any
continuous <i>f</i> function can be represented to any desired accuracy
given
sufficient parameters;  (2) the function <i>f</i>(<b>x</b>,<b>u</b>)<i>
</i>can
be evaluated quickly;  (3)&#160;
<IMG SRC="imgs/baird42.gif">
can be found exactly in constant time after evaluating
<i>f</i>(<b>x</b>,<b>u</b>);  (4) wire fitting can incorporate any general
function approximation system.  These four properties are discussed and it
is
shown how wire fitting can be combined with a memory-based learning system
and
<i>Q</i>-learning to control an inverted-pendulum system.<p>
<p>
<p>
Key words:  reinforcement learning, wire fitting, maximization,
<i>Q-</i>learning, advantage updating


</BLOCKQUOTE>
