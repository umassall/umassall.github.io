<HTML><HEAD>
<TITLE>An Analysis of Temporal-Difference Learning with Function Approximation</TITLE></HEAD>
<BODY bgcolor="FFFFBB">
<H2>An Analysis of Temporal-Difference Learning with Function Approximation</H2>
<!nextperson><B>Tsitsiklis, John</B> , Ben Van Roy<blockquote><A HREF= http://lids.mit.edu/~jnt/td.ps>An Analysis of Temporal-Difference Learning with Function Approximation</A><BR>

<I> IEEE Transactions on Automatic Control,
 Vol. 42, No. 5, May 1997, pp. 674-690.  </I>

(Postscript - 2 MB )

<BR><BR><B>Abstract</B>: We discuss the temporal-difference learning algorithm, as applied to
approximating cost-to-go function of an infinite-horizon discounted
Markov chain.  The algorithm we analyze updates parameters of a linear
function approximator on-line, during a single endless trajectory of
an irreducible aperiodic Markov chain with a finite or infinite state
space.  We present a proof of convergence (with probability 1), a
characterization of the limit of convergence, and a bound on the 
resulting approximation error.  Furthermore, our analysis is based on
a new line of reasoning that provides new intuition about the dynamics
of temporal-difference learning.
<p>In addition to proving new and stronger positive results than those
previously available, we identify the significance of on-line updating
and potential hazards associated with the use of nonlinear function
approximators.  First, we prove that divergence may occur when updates
are not based on trajectories of the Markov chain.  This fact reconciles
positive and negative results that have been discussed in the literature,
regarding the soundness of temporal-difference learning.  Second, we
present an example illustrating the possibility of divergence when
temporal-difference learning is used in the presence of a nonlinear
function approximator.    

</BLOCKQUOTE>
