<HTML><HEAD>
<TITLE>HQ-Learning: Discovering Markovian subgoals for non-Markovian reinforcement
learning</TITLE></HEAD>
<BODY bgcolor="FFFFBB">
<H2>HQ-Learning: Discovering Markovian subgoals for non-Markovian reinforcement
learning</H2>
<!nextperson><B> Wiering, M.</B>,Schmidhuber, Jurgen<blockquote>
<a href=ftp://ftp.idsia.ch/pub/juergen/hq96.ps.gz>HQ-Learning: Discovering
Markovian subgoals for non-Markovian reinforcement learning</a><br>
<I> Technical Report IDSIA-95-96, October 1996 </I>

( gzipped Postscript - 111 KB )

<BR><BR><B>Abstract</B>: To solve partially observable Markov decision problems, we introduce
HQ-learning, a hierarchical extension of Q-learning.  HQ-learning
is based on an ordered sequence of subagents, each learning to identify
and solve a Markovian subtask of the total task.  Each agent learns
(1) an appropriate subgoal (though there is no intermediate, external
reinforcement for "good" subgoals), and (2) a Markovian policy, given
a particular subgoal.  Our experiments demonstrate: (a) The system can
easily solve tasks standard Q-learning cannot solve at all. (b) It
can solve partially observable mazes with more states than those used
in most previous POMDP work.  (c) It can quickly solve complex tasks
that require manipulation of the environment to free a blocked path
to the goal. 
</BLOCKQUOTE>
