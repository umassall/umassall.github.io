<HTML><HEAD>
<TITLE> Residual Algorithms: Reinforcement Learning with Function
       Approximation</TITLE></HEAD>
<BODY bgcolor="FFFFBB">
<H2> Residual Algorithms: Reinforcement Learning with Function
       Approximation</H2>
<!nextperson><B>Baird, Leemon</B><blockquote><A HREF= http://www.cs.cmu.edu/~baird/papers/residual/residual.ps> Residual Algorithms: Reinforcement Learning with Function
       Approximation</A><BR>

<I> Armand Prieditis & Stuart Russell, eds. Machine Learning:
       Proceedings of the Twelfth International Conference, 9-12 July, Morgan Kaufman
 Publishers, San Francisco, CA </I>

(Postscript - 780 KB )

<BR><BR><B>Abstract</B>: A number of reinforcement learning algorithms have been developed that
are guaranteed to converge to the optimal solution when used with
lookup tables.  It is shown, however, that these algorithms can easily
become unstable when implemented directly with a general function-approximation
system, such as a sigmoidal multilayer perceptron, a radial-basis-function 
system, a memory-based learning system, or even a linear function-approximation
system.  A new class of algorithms, <i>residual gradient</i> algorithms,
is proposed, which perform gradient descent on the mean squared Bellman
residual, guaranteeing convergence.  It is shown, however, that they
may learn very slowly in some cases.  A larger class of algorithms, <i>residual</i>
algorithms, is proposed that has the guaranteed convergence of the residual
gradient algorithms, yet can retain the fast learning speed of direct
algorithms.  In fact, both direct and residual gradient algorithms are
shown to be special cases of residual algorithms, and it is shown that 
residual algorithms can combine the advantages of each approach.  The
direct, residual gradient, and residual forms of value iteration, Q-learning,
and advantage learning are all presented.  Theoretical analysis is given
explaining the properties these algorithms have, and simulation results
are given that demonstrate these properties.
</BLOCKQUOTE>
