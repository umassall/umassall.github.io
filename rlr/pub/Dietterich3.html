<HTML><HEAD>
<TITLE>The MAXQ Method for Hierarchical Reinforcement Learning</TITLE></HEAD>
<BODY bgcolor="FFFFBB">
<H2>The MAXQ Method for Hierarchical Reinforcement Learning</H2>
<!nextperson><B>Dietterich, Thomas</B><blockquote><A HREF= ftp://ftp.cs.orst.edu/pub/tgd/papers/ml98-maxq.ps.gz>The MAXQ Method for Hierarchical Reinforcement Learning</A><BR>

<I> Proceedings of the International Conference on Machine Learning, 1998 </I>

( gzipped Postscript - 53Kb )

<BR><BR><B>Abstract</B>: This paper presents a new approach to hierarchical reinforcement
learning based on the MAXQ decomposition of the value function.  The
MAXQ decomposition has both a procedural semantics---as a subroutine
hierarchy---and a declarative semantics---as a representation of the
value function of a hierarchical policy.  MAXQ unifies and extends
previous work on hierarchical reinforcement learning by Singh,
Kaelbling, and Dayan and Hinton.  Conditions under which the MAXQ
decomposition can represent the optimal value function are derived.
The paper defines a hierarchical Q learning algorithm, proves its
convergence, and shows experimentally that it can learn much faster
than ordinary ``flat'' Q learning.  Finally, the paper discusses some
interesting issues that arise in hierarchical reinforcement learning
including the hierarchical credit assignment problem and
non-hierarchical execution of the MAXQ hierarchy.
</BLOCKQUOTE>
