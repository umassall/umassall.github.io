<HTML><HEAD>
<TITLE>Generalization in Reinforcement Learning: Safely
       Approximating the Value Function</TITLE></HEAD>
<BODY bgcolor="FFFFBB">
<H2>Generalization in Reinforcement Learning: Safely
       Approximating the Value Function</H2>
<!nextperson><B>Boyan, Justin</B> , Andrew Moore<blockquote><A HREF=
http://www.cs.cmu.edu/afs/cs.cmu.edu/project/reinforcement/papers/boyan.funapprox-dp.ps.Z>
Generalization in Reinforcement Learning: Safely
       Approximating the Value Function</A><BR>

<I> Proceedings of Neural Information Processings
       Systems 7, Morgan Kaufmann, January 1995 (8 pages)  </I>

(compressed Postscript - 743 KB )

<BR><BR><B>Abstract</B>: A straightforward approach to the curse of dimensionality in
reinforcement learning and dynamic programming is to replace the
lookup table with a generalizing function approximator such as a
neural net.  Although this has been successful in the domain of
backgammon, there is no guarantee of convergence.  In this paper,
we show that the combination of dynamic programming and function
approximation is not robust, and in even very benign cases, may
produce an entirely wrong policy.  We then introduce Grow-Support,
a new algorithm which is safe form divergence yet can still reap
the benefits of successful generalization.
</BLOCKQUOTE>
