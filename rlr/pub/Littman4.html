<HTML><HEAD>
<TITLE>A generalized
reinforcement-learning model: Convergence and applications</TITLE></HEAD>
<BODY bgcolor="FFFFBB">
<H2>A generalized
reinforcement-learning model: Convergence and applications</H2>
<!nextperson><B>Littman, Michael</B> , Csaba Szepesvári<blockquote><A HREF= http://www.cs.duke.edu/~mlittman/docs/ml96.ps>A generalized
reinforcement-learning model: Convergence and applications</A><BR>

<I> 
Proceedings of the Thirteenth International Conference on Machine
 Learning </I>

(Postscript - 170KB )

<BR><BR><B>Abstract</B>: Reinforcement learning is the process by which an autonomous agent
uses its experience interacting with an environment to improve its
behavior.  The Markov decision process (MDP) model is a popular way of
formalizing the reinforcement-learning problem, but it is by no means
the only way.  In this paper, we show how many of the important
theoretical results concerning reinforcement learning in MDPs extend
to a generalized MDP model that includes MDPs, two-player games and
MDPs under a worst-case optimality criterion as special cases.  The
basis of this extension is a stochastic-approximation theorem that
reduces asynchronous convergence to synchronous convergence.

Keywords: Reinforcement learning, Q-learning convergence, Markov games

</BLOCKQUOTE>
