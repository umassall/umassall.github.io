<HTML><HEAD>
<TITLE>A Bayesian Framework for Reinforcement Learning</TITLE></HEAD>
<BODY bgcolor="FFFFBB">
<H2>A Bayesian Framework for Reinforcement Learning</H2>
<!nextperson><B>Strens, Malcolm</B><blockquote><A HREF= http://uk.geocities.com/mjstrens/icml.pdf>A Bayesian Framework for Reinforcement Learning</A><BR>

<I> International Conference on Machine Learning, 2000 </I>

(pdf - 83KB)

<BR><BR><B>Abstract</B>: The reinforcement learning problem can be decomposed into two parallel types of inference: (i) estimating the parameters of a model for the
underlying process; (ii) determining behavior which maximizes return under the estimated model. Following Dearden, Friedman and Andre
(1999), it is proposed that the learning process estimates online the full posterior distribution over models. To determine behavior, a
hypothesis is sampled from this distribution and the greedy policy with respect to the hypothesis is obtained by dynamic programming. By using a different hypothesis for each trial appropriate exploratory and exploitative behavior is obtained. This Bayesian method always converges to the optimal policy for a stationary process with discrete states.

</BLOCKQUOTE>
