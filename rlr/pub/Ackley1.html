<HTML><HEAD>
<TITLE>Generalization and scaling in reinforcement learning</TITLE></HEAD>
<BODY bgcolor="FFFFBB">
<H2>Generalization and scaling in reinforcement learning</H2>
<!nextperson><B>Ackley, David</B> , Michael L. Littman<blockquote><A HREF= http://>Generalization and scaling in reinforcement learning</A><BR>

<I> www.cs.duke.edu/~mlittman/docs/nips-crbp.ps </I>

(Postscript - 140KB )

<BR><BR><B>Abstract</B>: In associative reinforcement learning, an environment generates input
vectors, a learning system generates possible output vectors, and a
reinforcement function computes feedback signals from the input-output
pairs.  The task is to discover and remember input-output pairs that
generate rewards.  Especially difficult cases occur when rewards are
rare, since the expected time for any algorithm can grow exponentially
with the size of the problem.  Nonetheless, if a reinforcement
function possesses regularities, and a learning algorithm exploits
them, learning time can be reduced below that of non-generalizing
algorithms.  This paper describes a neural network algorithm called
complementary reinforcement back-propagation (CRBP), and reports
simulation results on problems designed to offer differing
opportunities for generalization.


</BLOCKQUOTE>
