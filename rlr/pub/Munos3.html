<HTML><HEAD>
<TITLE>A Convergent Reinforcement Learning algorithm in the continuous case : the Finite-Element Reinforcement Learning</TITLE></HEAD>
<BODY bgcolor="FFFFBB">
<H2>A Convergent Reinforcement Learning algorithm in the continuous case : the Finite-Element Reinforcement Learning</H2>
<!nextperson><B>Munos, Remi</B><blockquote><A HREF= http://www.cs.cmu.edu/~munos/papers/icml96.ps>A Convergent Reinforcement Learning algorithm in the continuous case : the Finite-Element Reinforcement Learning</A><BR>

<I> International 
Conference on Machine Learning, 1996 </I>

(Postscript - 197Kb )

<BR><BR><B>Abstract</B>: 
 This paper presents a direct reinforcement learning algorithm, called Finite-Element
 Reinforcement Learning, in the continuous case, i.e. continuous state-space and time. The
 evaluation of the value function enables the generation of an optimal policy for reinforcement
 control problems, such as target or obstacle problems, viability problems or optimization
 problems. We propose a continuous formalism for the studying of reinforcement learning using the
 continuous optimal control framework, then we state the associated Hamilton-Jacobi-Bellman
 equation. First, we propose to approximate the value function by a numerical scheme based on a
 finite-element method. This generates a discrete Markov Decision Process, with finite state and
 control spaces, which can be solved by Dynamic Programming. The computation of this
 approximation scheme, in reinforcement learning terminology, belongs to the class of indirect
 learning methods. Then we present our direct learning algorithm which approximates the
 previous finite-element scheme and prove its convergence to the value function of the continuous
 problem.
</BLOCKQUOTE>
