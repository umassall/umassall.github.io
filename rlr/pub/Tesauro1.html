<HTML><HEAD>
<TITLE>Temporal Difference Learning and TD-Gammon</TITLE></HEAD>
<BODY bgcolor="FFFFBB">
<H2>Temporal Difference Learning and TD-Gammon</H2>
<!nextperson><B>Tesauro, Gerald</B><blockquote><A HREF= http://www.research.ibm.com/massdist/tdl.html>Temporal Difference Learning and TD-Gammon</A><BR>

<I> unpublished </I>

(HTML - )

<BR><BR><B>Abstract</B>: Ever since the days of Shannon's proposal for a chess-playing algorithm [12] and Samuel's checkers-learning program
[10] the domain of complex board games such as Go, chess, checkers, Othello, and backgammon has been widely
regarded as an ideal testing ground for exploring a variety of concepts and approaches in artificial intelligence and
machine learning. Such board games offer the challenge of tremendous complexity and sophistication required to play at
expert level. At the same time, the problem inputs and performance measures are clear-cut and well defined, and the
game environment is readily automated in that it is easy to simulate the board, the rules of legal play, and the rules
regarding when the game is over and determining the outcome. 
<p>
This article presents a game-learning program called TD-Gammon. TD-Gammon is a neural network that trains itself
to be an evaluation function for the game of backgammon by playing against itself and learning from the outcome.
Although TD-Gammon has greatly surpassed all previous computer programs in its ability to play backgammon, that
was not why it was developed. Rather, its purpose was to explore some exciting new ideas and approaches to
traditional problems in the field of reinforcement learning. 
</BLOCKQUOTE>
