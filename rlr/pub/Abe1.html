<HTML><HEAD>
<TITLE>Reinforcement learning with immediate rewards and linear hypotheses</TITLE></HEAD>
<BODY bgcolor="FFFFBB">
<H2>Reinforcement learning with immediate rewards and linear hypotheses</H2>
<!nextperson><B>Abe, Naoki</B> , Alan Biermann and Philip M. Long<blockquote><A HREF= http://www.comp.nus.edu.sg/~plong/publications/reinforcement.ps>Reinforcement learning with immediate rewards and linear hypotheses</A><BR>

<I> Algorithmica </I>

(Postscript - 350KB )

<BR><BR><B>Abstract</B>: We perform theoretical analysis of algorithms for reinforcement
learning with immediate rewards using linear function approximation.
We provide nearly optimal worst-case bounds on cumulative regret, which, informally, is the difference between the total reward obtained by the algorithm, and the best total reward that can be obtained without having to learn.  The bounds are worst-case in the
sense that they hold for any sequence of states, where it is assumed
that the actions of the algorithm do not affect future states.
</BLOCKQUOTE>
