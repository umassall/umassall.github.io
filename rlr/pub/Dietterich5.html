<HTML><HEAD>
<TITLE>State abstraction in MAXQ hierarchical reinforcement learning</TITLE></HEAD>
<BODY bgcolor="FFFFBB">
<H2>State abstraction in MAXQ hierarchical reinforcement learning</H2>
<!nextperson><B>Dietterich, Thomas</B><blockquote><A HREF= ftp://ftp.cs.orst.edu/pub/tgd/papers/tr-maxq-abstraction.ps.gz>State abstraction in MAXQ hierarchical reinforcement learning</A><BR>

<I> unpublished </I>

( gzipped Postscript - 102Kb )

<BR><BR><B>Abstract</B>: Many researchers have explored methods for hierarchical reinforcement
learning (RL) with <i>temporal abstractions</i>, in which abstract
actions are defined that can perform many primitive actions before
terminating.  However, little is known about learning with <i>state
abstractions,</i> in which aspects of the state space are ignored.  In
previous work, we developed the MAXQ method for hierarchical RL.  In
this paper, we define five conditions under which state abstraction
can be combined with the MAXQ value function decomposition.  We prove
that the MAXQ-Q learning algorithm converges under these conditions
and show experimentally that state abstraction is important for the
successful application of MAXQ-Q learning.
</BLOCKQUOTE>
