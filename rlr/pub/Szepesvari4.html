<HTML><HEAD>
<TITLE>General Framework for Reinforcement Learning </TITLE></HEAD>
<BODY bgcolor="FFFFBB">
<H2>General Framework for Reinforcement Learning </H2>
<!nextperson><B>Szepesvari, Csaba</B><blockquote><A HREF= http://iserv.iki.kfki.hu/pub/papers/icann95/szepes.greinf.ps.Z>General Framework for Reinforcement Learning </A><BR>

<I> Proceedings of ICANN'95 Paris, France, Oct. 1995, Vol. II., pp. 165-170  </I>

( gzipped Postscript - ?? )

<BR><BR><B>Abstract</B>: In this article we propose a general framework for sequential decision making. The framework is based on the observation that the derivation of the optimal behaviour under various decision criteria follows the same pattern: the cost of policies can be decomposed into the successive application of an operator that defines the related dynamic programming algorithm and this operator describes completely the structure of the decision problem. We take this mapping (the so called one step lookahead (OLA) cost mapping) as our starting point. This enables the unified treatment of various decision criteria (e.g. the expected value criterion or the worst-case criterion). The main result of this article says that under minimal conditions optimal stationary policies are greedy w.r.t. the optimal cost function and vice versa. Based on this result we feel that former results on reinforcement learning can be transferred to other decision criteria provided that the decision criterion is decomposable by an appropriate mapping. 


</BLOCKQUOTE>
