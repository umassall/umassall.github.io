<HTML><HEAD>
<TITLE>Average Reward Reinforcement Learning:
Foundations, Algorithms, and Empirical Results</TITLE></HEAD>
<BODY bgcolor="FFFFBB">
<H2>Average Reward Reinforcement Learning:
Foundations, Algorithms, and Empirical Results</H2>
<!nextperson><B>Mahadevan, Sridhar</B><blockquote><A HREF= http://www.cps.msu.edu/~mahadeva/papers/mlj-rl-si-paper.ps.Z>Average Reward Reinforcement Learning:
Foundations, Algorithms, and Empirical Results</A><BR>

<I> Machine Learning ,
Special Issue on Reinforcement Learning (edited by Leslie Kaebling), vol.
22, pp. 159-196, 1996. </I>

(compressed Postscript -  )

<BR><BR><B>Abstract</B>: This paper presents a detailed study of average reward reinforcement
learning, an undiscounted optimality framework that is more appropriate
for cyclical tasks than the much better studied discounted framework.
A wide spectrum of average reward algorithms are described, ranging from
synchronous dynamic programming methods to several (provably convergent)
asynchronous algorithms from optimal control and learning automata.
A general sensitive discount optimality metric called <i>n-discount-optimality</i>
is introduced, and used to compare the various algorithms.  The overview
also uncovers a surprising limitation shared by the different algorithms:
while several algorithms can provably generate <i>gain-optimal</i> policies
that maximize average reward, none of them can reliably filter these
to produce <i>bias-optimal</i> (or <i>T-optimal</i>) policies that
also maximize the finite reward to absorbing goal states.  This paper
also presents a detailed empirical study of R-learning, an average
reward reinforcement learning method, using two empirical testbeds:
a stochastic grid world domain and a simulated robot environment.  A
detailed sensitivity analysis of R-learning is carried out to test its
dependence on learning rates and exploration levels.  The results
suggest that R-learning is quite sensitive to exploration strategies,
and can fall into sub-optimal limit cycles.  The performance of R-learning
is also compared with that of Q-learning, the best studied discounted RL
method.  Here, the results suggest that R-learning can be fine-tuned
to give better performance than Q-learning in both domains.  
</BLOCKQUOTE>
