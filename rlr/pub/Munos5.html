<HTML><HEAD>
<TITLE>Finite-Element methods with local triangulation refinement for continuous Reinforcement Learning problems</TITLE></HEAD>
<BODY bgcolor="FFFFBB">
<H2>Finite-Element methods with local triangulation refinement for continuous Reinforcement Learning problems</H2>
<!nextperson><B>Munos, Remi</B><blockquote><A HREF= http://www.cs.cmu.edu/~munos/papers/ecml97ps.zip>Finite-Element methods with local triangulation refinement for continuous Reinforcement Learning problems</A><BR>

<I> European Conference on
 Machine Learning, 1997 </I>

(compressed Postscript - 283Kb )

<BR><BR><B>Abstract</B>: 
 This paper presents a reinforcement learning algorithm for generating an adaptive control for a
 continuous process. Like Dynamic Programming methods, reinforcement learning find the
 optimal control by building a function, called the value function, that estimates the best
 expectation of future rewards. The algorithm proposed here uses finite-elements methods for
 approximating this function. It is composed of two dynamics: the learning dynamics, called
 Finite-Element Reinforcement Learning, which estimates the values at the vertices of a
 triangulation defined upon the state space, and the structural dynamics, which refines the
 triangulation inside regions where the value function is irregular. This mesh refinement algorithm
 intends to solve the problem of the combinatorial explosion of the number of values to be
 estimated. A formalism for reinforcement learning in the continuous case is proposed, the
 Hamilton-Jacobi-Bellman equation is stated, then the algorithm is presented and applied to a
 simple two-dimensional target problem. 
</BLOCKQUOTE>
