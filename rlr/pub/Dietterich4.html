<HTML><HEAD>
<TITLE>Hierarchical Reinforcement Learning with the MAXQ Value
Function Decomposition</TITLE></HEAD>
<BODY bgcolor="FFFFBB">
<H2>Hierarchical Reinforcement Learning with the MAXQ Value
Function Decomposition</H2>
<!nextperson><B>Dietterich, Thomas</B><blockquote><A HREF= ftp://ftp.cs.orst.edu/pub/tgd/papers/mlj-maxq.ps.gz>Hierarchical Reinforcement Learning with the MAXQ Value
Function Decomposition</A><BR>

<I> journal version; under review </I>

( gzipped Postscript - 192Kb )

<BR><BR><B>Abstract</B>: This paper presents a new approach to hierarchical
reinforcement learning based on the MAXQ decomposition of the value
function.  The MAXQ decomposition has both a procedural semantics---as
a subroutine hierarchy---and a declarative semantics---as a
representation of the value function of a hierarchical policy.  MAXQ
unifies and extends previous work on hierarchical reinforcement
learning by Singh, Kaelbling, and Dayan and Hinton.  Conditions under
which the MAXQ decomposition can represent the optimal value function
are derived.  The paper defines a hierarchical Q learning algorithm,
proves its convergence, and shows experimentally that it can learn
much faster than ordinary ``flat'' Q learning.  These results and
experiments are extended to support state abstraction and
non-hierarchical execution.  The paper concludes with a discussion of
design tradeoffs in hierarchical reinforcement learning.
[This is a longer version of the ICML paper.]
</BLOCKQUOTE>
