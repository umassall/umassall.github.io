<HTML><HEAD>
<TITLE>Markov games as a framework for multi-agent reinforcement learning</TITLE></HEAD>
<BODY bgcolor="FFFFBB">
<H2>Markov games as a framework for multi-agent reinforcement learning</H2>
<!nextperson><B>Littman, Michael</B><blockquote><A HREF= http://www.cs.duke.edu/~mlittman/docs/ml94-final.ps>Markov games as a framework for multi-agent reinforcement learning</A><BR>

<I> Proceedings of the Eleventh International 
Conference on Machine Learning </I>

(Postscript - 83KB )

<BR><BR><B>Abstract</B>: In the Markov decision process (MDP) formalization of reinforcement
learning, a single adaptive agent interacts with an environment
defined by a probabilistic transition function.  In this solipsistic
view, secondary agents can only be part of the environment and are
therefore fixed in their behavior.  The framework of Markov games
allows us to widen this view to include multiple adaptive agents with
interacting or competing goals.  This paper considers a step in this
direction in which exactly two agents with diametrically opposed goals
share an environment.  It describes a Q-learning-like algorithm for
finding optimal policies and demonstrates its application to a simple
two-player game in which the optimal policy is probabilistic.


</BLOCKQUOTE>
