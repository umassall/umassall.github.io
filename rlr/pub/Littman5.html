<HTML><HEAD>
<TITLE>Learning
policies for partially observable environments: Scaling up</TITLE></HEAD>
<BODY bgcolor="FFFFBB">
<H2>Learning
policies for partially observable environments: Scaling up</H2>
<!nextperson><B>Littman, Michael</B> , Anthony Cassandra and Leslie Kaelbling<blockquote><A HREF= http://www.cs.duke.edu/~mlittman/docs/ml95.ps>Learning
policies for partially observable environments: Scaling up</A><BR>

<I> Proceedings of the Twelfth
International Conference on Machine Learning </I>

(Postscript - 315K )

<BR><BR><B>Abstract</B>: Partially observable Markov decision processes (POMDPs) model decision
problems in which an agent tries to maximize its reward in the face of
limited and/or noisy sensor feedback.  While the study of POMDPs is
motivated by a need to address realistic problems, existing techniques
for finding optimal behavior do not appear to scale well and have been
unable to find satisfactory policies for problems with more than a
dozen states.  After a brief review of POMDPs, this paper discusses
several simple solution methods and shows that all are capable of
finding near-optimal policies for a selection of extremely small
POMDPs taken from the learning literature.  In contrast, we show that
none are able to solve a slightly larger and noisier problem based on
robot navigation.  We find that a combination of two novel approaches
performs well on these problems and suggest methods for scaling to
even larger and more complicated domains.


</BLOCKQUOTE>
