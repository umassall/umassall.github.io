<HTML><HEAD>
<TITLE>Reinforcement Learning Algorithms for Average-Payoff Markovian
Decision Processes</TITLE></HEAD>
<BODY bgcolor="FFFFBB">
<H2>Reinforcement Learning Algorithms for Average-Payoff Markovian
Decision Processes</H2>
<!nextperson><B>Singh, Satinder</B><blockquote><A HREF= 
ftp://ftp.cs.colorado.edu/users/baveja/Papers/AAAI94.ps.gz>Reinforcement Learning Algorithms for Average-Payoff Markovian
Decision Processes</A><BR>

<I> Proceedings of the Twelth National Conference on Artificial 
Intelligence </I>

( gzipped Postscript - 85 KB )

<BR><BR><B>Abstract</B>: Reinforcement learning (RL) has become a central paradigm for
solving learning-control problems in robotics and artificial 
intelligence.  RL researchers have focussed almost exclusively on
problems where the controller has to maximize the <i>discounted</i>
sum of payoffs.  However, as emphasized by Schwartz (1993), in many
problems, e.g., those for which the optimal behavior is a limited
cycle, it is more natural and computationally advantageous to
formulate tasks so that the controller's objective is to maximize the
average payoff received per time step.  In this paper I derive 
<i>new average-payoff</i> RL algorithms as stochastic approximation
methods for solving the system of equations associated with the
<i>policy evaluation</i> and <i>optimal control</i> questions in
average-payoff RL tasks.  These algorithms are analogous to the 
popular TD and Q-learning algorithms already developed for the
discounted-payoff case.  One of the algorithms derived here is a
significant variation of Schwartz's R-learning algorithm.  Preliminary
empirical results are presented to validate these new algorithms. 
</BLOCKQUOTE>
