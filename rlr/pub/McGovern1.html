<HTML><HEAD>
<TITLE>Macro-Actions in Reinforcement Learning: An Empirical Analysis</TITLE></HEAD>
<BODY bgcolor="FFFFBB">
<H2>Macro-Actions in Reinforcement Learning: An Empirical Analysis</H2>
<!nextperson><B>McGovern, Amy</B> , Richard S. Sutton<blockquote><A HREF= http://www-anw.cs.umass.edu/~amy/mcgovern-techrpt-98-70.ps.gz>Macro-Actions in Reinforcement Learning: An Empirical Analysis</A><BR>

<I> technical report </I>

( gzipped Postscript - 440K )

<BR><BR><B>Abstract</B>:    Several researchers have proposed reinforcement learning methods
   that obtain advantages in learning by using temporally extended
   actions, or <i> macro-actions</i>, but none has carefully analyzed
   what these advantages are.  In this paper, we separate and analyze
   two advantages of using macro-actions in reinforcement learning:
   the effect on exploratory behavior, independent of learning, and
   the effect on the speed with which the learning process propagates
   accurate value information.  We empirically measure the separate
   contributions of these two effects in gridworld and simulated
   robotic environments.  In these environments, both effects were
   significant, but the effect of value propagation was larger.  We
   also compare the accelerations of value propagation due to
   macro-actions and eligibility traces in the gridworld environment.
   Although eligibility traces increased the rate of convergence to
   the optimal value function compared to learning with macro-actions
   but without eligibility traces, eligibility traces did not permit
   the optimal policy to be learned as quickly as it was using
   macro-actions.

</BLOCKQUOTE>
