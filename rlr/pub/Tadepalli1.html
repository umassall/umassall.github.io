<HTML><HEAD>
<TITLE>Scaling up average reward reinforcement learning by approximating
the domain models and the value function</TITLE></HEAD>
<BODY bgcolor="FFFFBB">
<H2>Scaling up average reward reinforcement learning by approximating
the domain models and the value function</H2>
<!nextperson><B>Tadepalli, Prasad</B> , DoKyeong Ok<blockquote><A HREF= http://www.cs.orst.edu/~tadepall/research/papers/local-linear-regression.ps>Scaling up average reward reinforcement learning by approximating
the domain models and the value function</A><BR>

<I> Proceedings of the Thirteenth International Conference on Machine Learning, pages 471-479.  Morgan Kaufmann, 1996 </I>

(Postscript -  )

<BR><BR><B>Abstract</B>: Almost all the work in Average-reward Reinforcement Learning (ARL) so
far has focused on table-based methods which do not scale to domains
with large state spaces.  In this paper, we propose two extensions to
a model-based ARL method called H-learning to address the scale-up
problem.  We extend H-learning to learn action models and reward
functions in the form of Bayesian networks, and approximate its value
function using local linear regression.  We test our algorithms on several
scheduling tasks for a simulated Automatic Guided Vehicle (AGV) and
show that they are effective in significantly reducing the space
requirement of H-learning and making it converge faster.  To the best
of our knowledge, our results are the first in applying function
approximation to ARL. 
</BLOCKQUOTE>
