<HTML><HEAD>
<TITLE>Self-Improving Factory Simulation using 
Continuous-Time Average-Reward Reinforcement Learning</TITLE></HEAD>
<BODY bgcolor="FFFFBB">
<H2>Self-Improving Factory Simulation using 
Continuous-Time Average-Reward Reinforcement Learning</H2>
<!nextperson><B>Mahadevan, Sridhar</B> , Nicholas Marchalleck, Tapas Das, and Abhijit Gosavi<blockquote><A HREF= http://www.cps.msu.edu/~mahadeva/papers/mlc97.ps.gz>Self-Improving Factory Simulation using 
Continuous-Time Average-Reward Reinforcement Learning</A><BR>

<I> Proceedings of the 14th International Conference on Machine Learning (IMLC '97), Nashville,
TN, July 1997.  </I>

( gzipped Postscript -  )

<BR><BR><B>Abstract</B>: Many factory optimization problems, from inventory control to 
scheduling and reliability, can be formulated as continuous-time
markov decision processes.  A primary goal in such problems is to
find a gain-optimal policy that minimizes the long-run average cost.
This paper describes a new average-reward algorithm called SMART for
finding gain-optimal policies in continuous time semi-Markov decision
processes.  The paper presents a detailed experimental study of SMART
on a large unreliable production inventory problem.  SMART outperforms
two well-known reliability heuristics from industrial engineering.
A key feature of this study is the integration of the reinforcement
learning algorithm directly into two commercial discrete-event 
simulation packages, ARENA and CSIM, paving the way for this approach
to be applied to many other factory optimization problems for which
there already exist simulation models.
</BLOCKQUOTE>
