<HTML><HEAD>
<TITLE>Process-Oriented Planning and Average-Reward Optimality </TITLE></HEAD>
<BODY bgcolor="FFFFBB">
<H2>Process-Oriented Planning and Average-Reward Optimality </H2>
<!nextperson><B>Boutilier, Craig</B> , Martin L. Puterman <blockquote>
<A HREF= 
http://www.cs.ubc.ca/spider/cebly/Papers/_download_/avg-reward.ps>
Process-Oriented Planning and Average-Reward Optimality </A><BR>

<I> IJCAI-95  </I>

(Postscript - 150 KB )

<BR><BR><B>Abstract</B>: We argue that many AI planning problems should be viewed as 
<i>process-oriented</i>, where the aim is to produce a policy or
behavior strategy with no termination condition in mind, as opposed
to <i>goal-oriented</i>.  The full power of Markov decision models,
adopted recently for AI planning, becomes apparent with process-oriented
problems.  The question of appropriate optimality criteria becomes
more critical in this case; we argue that <i>average-reward optimality</i>
is most suitable.  While construction of average-optimal policies
involves a number of subtelties and computational difficulties, 
certain aspects of the problem can be solved using compact action 
representations such as Bayes nets.  In particular, we provide an
algorithm that identifies a planning problem - a crucial element of
constructing average optimal policies - without explicit enumeration
of the problem state space.
</BLOCKQUOTE>
