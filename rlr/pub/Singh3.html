<HTML><HEAD>
<TITLE>Reinforcement Learning with a Hierarchy of Abstract Models</TITLE></HEAD>
<BODY bgcolor="FFFFBB">
<H2>Reinforcement Learning with a Hierarchy of Abstract Models</H2>
<!nextperson><B>Singh, Satinder</B><blockquote><A HREF= ftp://ftp.cs.colorado.edu/users/baveja/Papers/AAAI92.ps.gz>Reinforcement Learning with a Hierarchy of Abstract Models</A><BR>

<I> Appears in Proceedings of the Tenth National Conference on Artificial Intelligence,
1992 </I>

( gzipped Postscript - 105 KB )

<BR><BR><B>Abstract</B>: Reinforcement learning (RL) algorithms have traditionally been thought
of as trial and error learning methods that use actual control
experience to incrementally improve a control policy.  Sutton's DYNA
architecture demonstrated that RL algorithms can work as well using
simulated experience from an environment model, and that the resulting
computation was similar to doing one-step lookahead planning.  I propose
learning a hierarchy of models of the environment that abstract
temporal detail as a means of improving the scalability of RL algorithms.
I present H-DYNA (Hierarchical DYNA), an extension to Sutton's DYNA
architecture that is able to learn such a hierarchy of abstract models.
H-DYNA differs from hierarchical planners in two ways: first, the 
abstract models are learned using experience gained while learning to
solve other tasks in the same environment, and second, the abstract
models can be used to solve stochastic control tasks.  Simulations
on a set of compositionally-structured navigation tasks show that
H-DYNA can learn to solve them faster than conventional RL algorithms.
The abstract models also serve as mechanisms for achieving transfer
of learning across multiple tasks.
</BLOCKQUOTE>
