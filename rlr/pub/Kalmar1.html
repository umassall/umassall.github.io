<HTML><HEAD>
<TITLE>Module-Based Reinforcement Learning for a Real Robot 

</TITLE></HEAD>
<BODY bgcolor="FFFFBB">
<H2>Module-Based Reinforcement Learning for a Real Robot 

</H2>
<!nextperson><B>Kalmar, Zsolt</B> , Cs. Szepesvari and A. Lorincz<blockquote><A HREF= http://sneaker.mindmaker.kfkipark.hu/~szepes/papers/ewlrlr97.ps.gz>Module-Based Reinforcement Learning for a Real Robot 

</A><BR>

<I> Proceedings of the 6th European Workshop on Learning Robots, Lecture Notes in AI, to appear. 1998 </I>

( gzipped Postscript - 618 KB )

<BR><BR><B>Abstract</B>: The behaviour of reinforcement learning (RL) algorithms is best understood in completely observable, finite state- and action-space, discrete-time controlled Markov-chains. Robot-learning domains, on the other hand, are inherently infinite both in time and space, and moreover they are only partially observable. In this article we suggest a systematic design method whose motivation comes from the desire to transform the task-to-be-solved into a finite-state, discrete-time, epsilon-stationary Markovian task, which is completely observable too. 

An epsilon-stationary MDP is one whose transition probabilities may vary by time, but remain in an epsilon-neighorhood of some ideal transition probability matrix, p*. We prove that model-based RL algorithms converge to a neighborhood of the optimal value-function corresponding to p*, whith the neighborhood size being proportional to epsilon and some other parameters of the MDP.

The approach was tried out on a real-life robot. Several RL algorithms were compared and it was found that a model-based approach worked best. The learnt switching strategy performed equally well as a handcrafted version. Moreover, the learnt strategy seemed to exploit certain properties of the environment which could not have been seen in advance, which predicted the promising possibility that a learnt controller might overperform a handcrafted switching strategy in the future.


</BLOCKQUOTE>
