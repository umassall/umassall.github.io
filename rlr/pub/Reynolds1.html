<HTML><HEAD>
<TITLE>The Stability of General Discounted Reinforcement Learning with Linear Function Approximation</TITLE></HEAD>
<BODY bgcolor="FFFFBB">
<H2>The Stability of General Discounted Reinforcement Learning with Linear Function Approximation</H2>
<!nextperson><B>Reynolds, Stuart</B><blockquote><A HREF= http://cs.bham.ac.uk/~sir/pub/ukci-02.ps.gz>The Stability of General Discounted Reinforcement Learning with Linear Function Approximation</A><BR>

<I> UKCI'02 </I>

( gzipped Postscript - 80 )

<BR><BR><B>Abstract</B>:   This paper shows that general discounted return estimating
  reinforcement learning algorithms cannot diverge to infinity when a
  form of linear function approximator is used for approximating the
  value-function or Q-function. The results are significant insofar
  as examples of divergence of the value-function exist where similar
  linear function approximators are trained using a similar
  incremental gradient descent rule. A different gradient descent
  error criterion is used to produce a training rule which has a 
    non-expansion property and therefore cannot possibly diverge.
  This training rule is found to be commonly used for reinforcement
  learning.
</BLOCKQUOTE>
