<HTML><HEAD>
<TITLE>Approximating optimal policies for partially observable stochastic
domains
</TITLE></HEAD>
<BODY bgcolor="FFFFBB">
<H2>Approximating optimal policies for partially observable stochastic
domains
</H2>
<!nextperson><B>Parr, Ronald</B> , Stuart Russell<blockquote><A HREF=
http://http.cs.berkeley.edu/~russell/papers/ijcai95-porl.ps>Approximating 
optimal policies for partially observable stochastic
domains
</A><BR>

<I> Proceedings of the IJCAI, 1995 </I>

(Postscript - 157 KB )

<BR><BR><B>Abstract</B>: The problem of making optimal decisions in uncertain conditions is
central to Artificial Intelligence.  If the state of the world is
known at all times, the world can be modeled as a Markov Decision 
Process (MDP).  MDPs have been studied extensively and many methods
are known for determining optimal courses of action, or policies.
The more realistic case where state information is only partially
observable, Partially Observable Markov Decision Processes (POMDPs),
have received much less attention.  The best exact algorithms for
these problems can be very inefficient in both space and time.  We
introduce Smooth Partially Observable Value Approximation (SPOVA),
a new approximation method that can quickly yield good approximations
which can improve over time.  This method can be combined with
reinforcement learning methods, a combination that was very effective
in our test cases. 
</BLOCKQUOTE>
