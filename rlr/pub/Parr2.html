<HTML><HEAD>
<TITLE>ierarchical Control and Learning for Markov Decision Processes</TITLE></HEAD>
<BODY bgcolor="FFFFBB">
<H2>ierarchical Control and Learning for Markov Decision Processes</H2>
<!nextperson><B>Parr, Ronald</B><blockquote><A HREF= http://robotics.stanford.edu/~parr/thesis600.ps.gz>ierarchical Control and Learning for Markov Decision Processes</A><BR>

<I> PhD Thesis </I>

( gzipped Postscript -  )

<BR><BR><B>Abstract</B>: 

This dissertation investigates the use of hierarchy and problem decomposition as a means of solving large, stochastic, sequential decision problems.
These problems are framed as Markov decision problems (MDPs). The new technical content of this dissertation begins with a discussion of the concept
of temporal abstraction. Temporal abstraction is shown to be equivalent to the transformation of a policy defined over a region of an MDP to an action in
a semi-Markov decision problem (SMDP). Several algorithms are presented for performing this transformation efficiently. 

This dissertation introduces the HAM method for generating hierarchical, temporally abstract actions. This method permits the partial specification of
abstract actions in a way that corresponds to an abstract plan or strategy. Abstract actions specified as HAMs can be optimally refined for new tasks by
solving a reduced SMDP. The formal results show that traditional MDP algorithms can be used to optimally refine HAMs for new tasks. This can be
achieved in much less time than it would take to learn a new policy for the task from scratch. 
</BLOCKQUOTE>
