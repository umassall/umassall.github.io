<HTML><HEAD>
<TITLE>REINFORCEMENT LEARNING AND POMDPs (dozens of papers on RL in partially observable environments since 1989)
</TITLE></HEAD>
<BODY bgcolor="FFFFBB">
<H2>REINFORCEMENT LEARNING AND POMDPs (dozens of papers on RL in partially observable environments since 1989)
</H2>
<!nextperson><B>Schmidhuber, Juergen</B><blockquote><A HREF= http://www.idsia.ch/~juergen/rl.html>REINFORCEMENT LEARNING AND POMDPs (dozens of papers on RL in partially observable environments since 1989)
</A><BR>

<I> Journal papers and conference papers </I>

(HTML - 100KB)

<BR><BR><B>Abstract</B>: Realistic environments are not fully observable. General learning agents need an internal state to memorize important events. The essential question is: how can they learn to identify and store those events relevant for further optimal action selection? To address this issue, we have studied reinforcement learners with recurrent neural network value function approximators (1990 -), recurrent network world models (1990 -), actions that address and set internal storage cells, trained by the success-story algorithm (1994 -), direct search in a space of event-memorizing algorithms (1994 -), and Goedel machines (2003 -). 
</BLOCKQUOTE>
