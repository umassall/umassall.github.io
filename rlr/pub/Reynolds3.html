<HTML><HEAD>
<TITLE>Experience Stack Reinforcement Learning for Off-Policy Control</TITLE></HEAD>
<BODY bgcolor="FFFFBB">
<H2>Experience Stack Reinforcement Learning for Off-Policy Control</H2>
<!nextperson><B>Reynolds, Stuart</B><blockquote><A HREF= http://www.cs.bham.ac.uk/~sir/pub/EScolour-CSRP-02-1.ps.gz>Experience Stack Reinforcement Learning for Off-Policy Control</A><BR>

<I> Cognitive Science Technical Report number CSRP-02-1, School of Computer Science, The University of Birmingham, Birmingham, B15 2TT, UK. January 2002 </I>

( gzipped Postscript - 235 )

<BR><BR><B>Abstract</B>: This paper introduces a novel method for allowing backwards replay to be applied as an online learning algorithm. The general technique can be adapted to provide analogues of most existing eligibility trace algorithms. The new method remains as computationally cheap as current techniques but, as it directly employs lambda-return estimates in value updates, remains significantly simpler. The paper concentrates on multi-step off-policy control methods (such as Watkins' Q(lambda)) as a theoretically and practically important class of algorithms that are underutilised in practice. Experimental results show improvements upon existing eligibility trace methods across a wide range of parameter settings and also highlight the importance of the initial Q-function upon the performance of several reinforcement learning algorithms. 
</BLOCKQUOTE>
