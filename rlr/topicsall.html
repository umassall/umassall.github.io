<html>
<head>
<title>Reinforcement Learning Repository at MSU - Topics</title>
<script type="text/javascript">

  var _gaq = _gaq || [];
  _gaq.push(['_setAccount', 'UA-35723838-1']);
  _gaq.push(['_trackPageview']);

  (function() {
    var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
    ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
    var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
  })();

</script>
</head>
<body bgcolor="FFFFBB">
<base target="rl-main">
</head>

<body bgcolor="FFFFBB">
<h3>Reinforcement Learning Repository at MSU</h3>
<img src="imgs/bookmark.gif">
<b><font size=6>Topics in Reinforcement Learning</font></b>
<font size=6>
<p>Reinforcement learning is the area of machine learning which addresses
how an agent can learn behavior through interaction with its environment.  
The term reinforcement learning has its roots in behavioral psychology,
which is based on the theory that humans learn by trial-and-error
interaction, and subsequent rewards and punishments.  
<p>Reinforcement learning differs from the well-studied problem of
supervised learning in two ways.  First, the reinforcement learner is not
trained with input/output pairs, but rather, the agent's task is to gather
useful information about its environment.  Second, the learning is
on-line; learning and performance evaluation may be concurrent. 
<br><br>

<li><b>Applications to Robotics</b>: &nbsp
Robot problems represent a particular set of challenges to RL because of 
noisy sensors and effectors and multi-level behavior representation.   RL
has been applied to robot navigation and behavior coordination
<br>
   <a href=ar-top.html>overview</a> &nbsp
   <a href=ar.html>publications</a> &nbsp
<p>
<li><b>Average-reward/Undiscounted Methods</b>: &nbsp
The value function to be learned based on a particular policy is often the
discounted cumulative reward.  However, another possibility is average
reward, per time step over the entire lifetime of the agent.  
<br><a href=un-top.html>overview</a> &nbsp
    <a href=un.html>publications</a> &nbsp
<p>
<li><b>DP/MDP</b>: &nbsp
Dynamic programming (DP) reinforcement learning methods are a class of
problems in which optimal policies are computed based on a given Markov
decision process
(MDP) model of the environment.  DP methods use value functions to
structure the search for optimal policies.  Two basic approaches to this
search are policy iteration and value iteration.  Much current research
focuses on comparing, modifying, and extending these approaches.
<p>Because DP methods are based
on MDPs, they require a complete and accurate model of the environment.

An alternative to DP/MDP methods is to search in the space of behaviors,
which is the approach taken in genetic algorithms.

References:

An extensive discussion of MDPs may be found in:

Puterman, M.  Markov Decision Processes: Discrete Dynamic Stochastic
Programming.  John Wiley, 1994.

<br>    <a href=dp-top.html>overview</a> &nbsp
    <a href=dp.html>publications</a> &nbsp
<p>
<li><b>Function Approximation</b>: &nbsp
"To solve problems with continuous or large finite state spaces, it is
crucial to
approximate the value function during reinforcement learning or dynamic
programming. A number
of techniques work well in practice, however, the theoretical properties
of these methods are not yet well understood.  In addition, there are some
situations for which the existing methods are not suited; new algorithms
must be developed." (Report of the 1996 Workshop on Reinforcement
Learning, Mahadevan and Kaelbling)


<br>    <a href=fa-top.html>overview</a> &nbsp
    <a href=fa.html>publications</a> &nbsp
<p>
<li><b>Hierarchical Methods</b>: &nbsp
Hierarchical methods represent a strategy for dealing with very large
state spaces.  The motivation for the use of hierarchical models in RL is
the goal of faster learning (perhaps at the expense of slight
sub-optimality in performance) through decomposition of a task into a
collection of simpler subtasks.     

<br>    <a href=hm-top.html>overview</a> &nbsp
    <a href=hm.html>publications</a> &nbsp
<p>
<li><b>Industrial Applications</b>: &nbsp
RL has been applied to several problems in industrial process
control.  These include packaging control, optimal scheduling of a
multiple-elevator system, and job-shop scheduling (scheduling of a set of
tasks to satisfy a set of both temporal and resource constraints).
<br>    <a href=in-top.html>overview</a> &nbsp
    <a href=in.html>publications</a> &nbsp
<p>
<li><b>Neuro-biological RL</b>: &nbsp
The neuro-biological approach to RL "addresses the question of how animals
learn to predict which stimuli and actions are associated with rewards and
how they change their policies accordingly." (M&K'96 wrkshp)  The goal is
to translate this understanding to a computational theory for reward
learning.
<br>    <a href=nb-top.html>overview</a> &nbsp
    <a href=nb.html>publications</a> &nbsp
<p>
<li><b>Partially-observable Problems</b>: &nbsp
The vast majority of RL work concentrates on the completely observable
case, in which there is noise in the agent's actions, but none in its
observation of the environment.  

When the state space is not completely observable, additional methods need
to be used beyond those which apply to MDPs.  Partially observable 
problems typically use probabilistic methods such as Bayesian belief
networks.  

<br>    <a href=po-top.html>overview</a> &nbsp
    <a href=po.html>publications</a> &nbsp
<p>
<li><b>Planning</b>: &nbsp
The issue of planning is one that is common to Artificial Intelligence
methods: an agent must devise a sequence of actions that leads from an
initial state to a goal state.  However, in the context of RL, planning is
often required to be done under uncertainty.  This arises from several
sources: incomplete knowledge of states and actions, and potentially
conflicting objectives.  

<br>    <a href=pl-top.html>overview</a> &nbsp
    <a href=pl.html>publications</a> &nbsp
<p>
<li><b>Shaping</b>: &nbsp
<br>"The technique of shaping is used in training animals; a teacher
presents very simple problems to solve first, then gradually exposes the
learner to more complex problems.  Shaping has been used in
supervised-learning systems, and can be used to train hierarchical
reinforcement-learning systems from the bottom up, and to alleviate
problems of delayed reinforcement by decreasing the delay until the
problem is well understood." (Kaelbling, Littman, and Moore, 1996)   
<br>
<a href=sh-top.html>overview</a> &nbsp
    <a href=sh.html>publications</a> &nbsp
<p>
<li><b>TD-learning</b>: &nbsp
Temporal difference (TD) learning methods are a class of problems which
learn by iteratively reducing the discrepancies between the estimates
produced
by the agent at different times. TD methods do not require a model.
Q learning is a widely used TD method.
<br>    <a href=td-top.html>overview</a> &nbsp
    <a href=td.html>publications</a> &nbsp
<p>
<li><b>Theoretical analysis</b>: &nbsp
<br>    <a href=th-top.html>overview</a> &nbsp
    <a href=th.html>publications</a> &nbsp


