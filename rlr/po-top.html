<html>
<head>
<title>Reinforcement Learning Repository at MSU</title>
<script type="text/javascript">

  var _gaq = _gaq || [];
  _gaq.push(['_setAccount', 'UA-35723838-1']);
  _gaq.push(['_trackPageview']);

  (function() {
    var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
    ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
    var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
  })();

</script>
</head>
<body bgcolor="FFFFBB">
<base target="rl-main">
</head>

<body bgcolor="FFFFBB">
<h3>Reinforcement Learning Repository at MSU</h3>
<img align=left src="imgs/bookmark.gif">
<font size=6><b>Topics: Partially Observable Problems</font></b>

<br>
<br><br><br>


<p>
The vast majority of RL work
concentrates on the completely observable case, in which there is
noise in the agent's actions, but none in its observation of the   
environment. When the state space is not completely observable,
additional methods need to be used beyond those which apply to
MDPs. Partially observable Markov decision processes (POMDP's) extend
the MDP model to include an observational model (which is a stochastic
function of the current state). The true state is hidden because the
number of observations is typically far less than the number of
possible states.

<br><br>
   <a href=po.html>publications</a> &nbsp
