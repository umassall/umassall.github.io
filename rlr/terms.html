<html>
<head>
<title>Glossary of Terminology in Reinforcement Learning</title>
<script type="text/javascript">

  var _gaq = _gaq || [];
  _gaq.push(['_setAccount', 'UA-35723838-1']);
  _gaq.push(['_trackPageview']);

  (function() {
    var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
    ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
    var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
  })();

</script>
</head>
<body bgcolor="FFFFBB">
<img align=right src="imgs/tower.gif">
<h2>Glossary of Terminology in Reinforcement Learning</h2>

<p><b>actor-critic</b> - Refers to a class of agent architectures,
where the actor plays out a particular policy, while the critic learns
to evaluate actor's policy. Both the actor and critic are
simultaneously improving by bootstrapping on each other. 

<p><b>agent</b> - A system that is embedded in an environment, and
takes <i> actions </i> to change the state of the environment.
Examples include mobile robots, software agents, or industrial
controllers.

<p><b>average-reward methods</b> - A framework where the agent's goal
is to maximize the expected payoff per step. Average-reward methods
are appropriate in problems where the goal is maximize the long-term
performance. They are usually much more difficult to analyze than
discounted algorithms. 

<p><b>discount factor</b> - A scalar value between 0 and 1 which
determines the present value of future rewards.  If the discount
factor is 0, the agent is concerned with maximizing immediate rewards.
As the discount factor approaches 1, the agent takes more future
rewards into account. Algorithms which discount future rewards include
Q-learning and TD(lambda).

<p><b>dynamic programming (DP) </b> is a class of solution methods for
solving sequential decision problems with a compositional cost
structure. Richard Bellman was one of the principal founders of this
approach.

<p><b>environment</b> - The external system that an agent is
"embedded" in, and can perceive and act on.

<p><b>function approximator</b> refers to the problem of inducing a
function from training examples.  Standard approximators include
decision trees, neural networks, and nearest-neighbor methods. 

<p><b>Markov decision process (MDP)</b> - A probabilistic model of a
sequential decision problem, where states can be perceived exactly,
and the current state and action selected determine a probability
distribution on future states. Essentially, the outcome of applying an
action to a state depends only on the current action and state (and
not on preceding actions or states).

<p><b>model-based algorithms</b> - These compute value functions using
a model of the system dynamics. Adaptive Real-time DP (ARTDP) is a
well-known example of a model-based algorithm. 

<p><b>model-free algorithms</b> - these directly learn a value
function without requiring knowledge of the consequences of doing
actions. Q-learning is the best known example of a model-free
algorithm.

<p><b>model</b> - The agent's view of the environment, which maps
state-action pairs to probability distributions over states.  Note that
not every reinforcement learning agent uses a model of its environment.

<p><b>Monte Carlo methods</b> - A class of methods for learning of
value functions, which estimates the value of a state by running many
trials starting at that state, then averages the total rewards
received on those trials.

<p><b>policy</b> - The decision-making function (control strategy) of the
agent, which represents a mapping from situations to actions. 

<p><b>reward</b> - A scalar value which represents the degree to which
a state or action is desirable. Reward functions can be used to
specify a wide range of planning goals (e.g. by penalizing every
non-goal state, an agent can be guided towards learning the fastest
route to the final state).

<p><b>sensor</b> - Agents perceive the state of their environment
using sensors, which can refer to physical transducers, such as
ultrasound, or simulated feature-detectors.

<p><b>state</b> - this can be viewed as a summary of the past history
of the system, that determines its future evolution.

<p><b>stochastic approximation</b> - Most RL algorithms can be viewed
as stochastic approximations of exact DP algorithms, where instead of
complete sweeps over the state space, only selected states are backed
up (which are sampled according to the underlying probabilistic
model). A rich theory of stochastic approximation (e.g Robbins Munro)
can be brought to bear to understand the theoretical convergence of RL
methods.

<p><b>TD (temporal difference) algorithms</b> - A class of learning
methods, based on the idea of comparing temporally successive
predictions. Possibly the single most fundamental idea in all of
reinforcement learning.

<p><b>unsupervised learning</b> - The area of machine learning in
which an agent learns from interaction with its environment, rather
than from a knowledgeable teacher that specifies the action the agent
should take in any given state.

<p><b>value function</b> is a mapping from states to real numbers,
where the value of a state represents the long-term reward achieved
starting from that state, and executing a particular policy. The key
distinguishing feature of RL methods is that they learn policies
indirectly, by instead learning value functions. RL methods can be
constrasted with direct optimization methods, such as genetic
algorithms (GA), which attempt to search the policy space directly. 



