<HTML>
<HEAD>
</HEAD>
<BODY TEXT="#000000" BGCOLOR="#FFFFFF" LINK="#FF0000" VLINK="#800080" ALINK="#0000FF">

<H1><IMG SRC="images/logob.gif" HSPACE=20 HEIGHT=64 WIDTH=64 ALIGN=CENTER>
WebSim </H1>

<H5>All of this code is (c) 1996 by the respective authors, is freeware,
and may be freely distributed. If modifications are made, please say so
in the comments.</H5>

<P>
<HR WIDTH="100%">This page requires a Java-aware browser and may take several
minutes to download.
<HR WIDTH="100%"></P>

<H1 ALIGN=CENTER>Simulation Definition:</H1>

<P><B>See the source HTML code for this page to see the simulation definition
that is parsed and executed by the <A HREF="../WebSim/index.html">WebSim</A>
applet.<BR>
<BR>
<FONT SIZE=+1>MDP</FONT>:</B> A 2D grid [0..1,0..1] discretized into units
of 0.2. Four actions are possible:&nbsp;<FONT SIZE=+0>0 - increment the
current x coordinate by 0.2; 0.25 - increment the current y coordinate
by 0.2; 0.5 - decrement the current x coordinate by 0.2; 0.75 - decrement
the current y coordinate by 0.2</FONT>. State [0,0] is the initial state.
State [1,1] is an absorbing state, has a defined value of 0 and returns
a -1 reinforcement. All other state transitions return a reinforcement
of 1. States on the boundaries of state space (for example [1,0.5])&nbsp;have
only three legal moves instead of four. The objective is to find a path
to the goal state that minimizes the reinforcement received.<BR>
<BR>
<B><FONT SIZE=+1>Function Approximator</FONT>:</B> <FONT SIZE=+0>a lookup table.<BR>
<BR>
</FONT><B><FONT SIZE=+1>Learning algorithm</FONT>:</B> Backprop<BR>
<BR>
<B><FONT SIZE=+1>RL algorithm</FONT>: </B>Residual Gradient QLearning<BR>
<BR>
<B><FONT SIZE=+1>Displays</FONT>: <BR>
</B><FONT SIZE=+1>1)</FONT><FONT SIZE=+0> Variables and Rates (upper left
corner)<BR>
</FONT><FONT SIZE=+1>2)</FONT><FONT SIZE=+0> 2D graph of log error vs.
learning time (upper right corner)<BR>
</FONT><FONT SIZE=+1>3)</FONT><FONT SIZE=+0> 3D graph of value function
(lower left corner)<BR>
</FONT><FONT SIZE=+1>4)</FONT><FONT SIZE=+0> 3D graph of policy</FONT></P>

The 3D graphs can be rotated on two different axis by clicking
and dragging inside or outside of the box.
</P>

<P><FONT SIZE=+0><B>Value Function Display:</B>&nbsp;Remember that the
value of a state is the sum of the reinforcements received when starting
in that state and performing successive transitions until the absorbing
state is reached. The X-axis and Y-axis correspond to state space. The
Z-axis (height) is the value (minimum Q-value) in each state.  The learned
function should look like a stepped slope with the lowest point at [1,1] and
the highest point at [0,0].</FONT></P>

<P><FONT SIZE=+0><B>Policy Display:</B> The X-axis and Y-axis correspond
to state space. The Z-axis (height) is the action considered best in each
state:&nbsp;0 - increment the current x coordinate by 0.2; 0.25 - increment
the current y coordinate by 0.2; 0.5 - decrement the current x coordinate
by 0.2; 0.75 - decrement the current y coordinate by 0.2.  The learned policy
is correct when all states (with the exception of the [1,y] and [x,1] rows) have
a value of 0 or 0.25.  The [1,y] row should have a value of 0.25, and the [x,1]
row should have a value of 0.</FONT></P>

<P><FONT SIZE=+0><B>Suggestions for Experiments:</B>&nbsp;Change the value
of gamma and observe the resulting change in the optimal value function.
Click on the # symbol by the WebSim logo.  Change the &quot;gamma&quot; parameter.
Remember that it may be necessary to decrease the
value of the learning rate parameter &quot;rate&quot; for larger values
of gamma. Click <A HREF="../index.html">here</A>
to find a more complete description of <A HREF="../index.html">WebSim(c)</A>
and how it can be used to perform experiments for many different RL&nbsp;algorithms
and MDPs.</FONT></P>


<P><APPLET codebase="../classes" code=WebSim.class width=620 height=620><PARAM name=sourceText value="
`/*****************************************************************/
`//unparse
`embed
`(0,0,524,60)
`Simulator {
`  experiment Backprop {
`    learningRate  0.3
`    momentum  0.0
`    tolerance -1 //this cannot be set to 0 if smooth is set to 0 when using a lookup table.  The error of a single sample might be 0.
`    smooth    0.9
`    error ReinforcementLearning {
`      algorithm       QLearning
`      method          direct
`      dt              0.2
`      gamma           0.9
`      incremental     true
`      mdp             GridWorld  granularity 5
`      funApp #DEF FUNCTION {LookupTable  {0.0 1.0 1}  //this tuple is for the bias element of the input (will go away when RL interface is updated)
`                                         {0.0 1.0 6}  //there are 3 triplets, each corresponding to a dimension of the
`                                         {0.0 1.0 6}  //input vector.  The elements of the tuples are ordered
`                                         {0.0 0.75 4}} //{min, max, #elements}.   The inputs are x coord, y coord, action.
`    }
`  }
`  displays {
`    embed
`    (0,0,250,200)
`    title {title 'Learned policy vs. state' display
`    Graph3D {
`                trigger 'time' freq 100
`                inputs  [1 0 0 0] //this now reflects the input to the function, not ValuePolicy.  This will change
`                xElement 1 xSamples 13 xMin 0 xMax 1  //when the interfaces are updated
`                yElement 2 ySamples 13 xMin 0 xMax 1
`                zElement 1  //the action element of the output vector
`                function #DEF POLICY {ValuePolicy {
`                                       mdp GridWorld
`                                       funApp #USE FUNCTION
`                                      }}
`                plotFloor true   //plotTop true
`                floorColor [0,0,0]
`                //plots {
`                //  Contour { contours 10
`                //            spectrum true
`                //            xElement  1   xSamples 10
`                //            yElement  2   ySamples 10
`                //            zElement  0
`                //            function #USE POLICY
`                //  }
`                //}
`    }
`    }
`    embed
`    (0,0,250,200)
`    title {title 'Learned value vs. state' display
`    Graph3D {
`                trigger 'time' freq 100
`                inputs  [1 0 0 0]
`                xElement 1 xSamples 11 xMin 0 xMax 1
`                yElement 2 ySamples 11 xMin 0 xMax 1
`                zElement 0 //the value element of the output vector
`                function #DEF POLICY {ValuePolicy {
`                                       mdp GridWorld
`                                       funApp #USE FUNCTION
`                                      }}
`                plotFloor true   //plotTop true
`                floorColor [0,0,0]
`                //plots {
`                //   Contour { contours 10
`                //            spectrum true
`                //            xElement  1   xSamples 10
`                //            yElement  2   ySamples 10
`                //            zElement  0
`                //            function #USE POLICY
`                //  }
`                //}
`    }
`    }
`     embed
`     (0,0,200,200)
`     title {title 'Log error vs. timestep' display
`     Graph2D {
`       trigger 'time' freq 100
`       plots {
`         plotXY {
`           size 100
`           lineColor [0 .5 0] symbolColor [0 .8 0]
`           trigger 'time' freq 100
`           x       'time'
`           y       'log error'
`         }
`       }
`     }
`     }
`  }
`}
`/*****************************************************************/
">
<HR>This Java applet requires a Java-aware browser such as Netscape 2.0
for Solaris/Win95/WinNT.
<HR></APPLET></P>

</BODY>
</HTML>
</APPLET></P>

</BODY>
</HTML>
