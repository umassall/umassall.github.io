<HTML>
<HEAD>
</HEAD>
<BODY TEXT="#000000" BGCOLOR="#FFFFFF" LINK="#FF0000" VLINK="#800080" ALINK="#0000FF">

<H1><IMG SRC="images/logob.gif" HSPACE=20 HEIGHT=64 WIDTH=64 ALIGN=CENTER>
WebSim</H1>

<H5><BR>
All of this code is (c) 1996 by the respective authors, is freeware, and
may be freely distributed. If modifications are made, please say so in
the comments.</H5>

<P>
<HR WIDTH="100%">This page requires a Java-aware browser and may take several
minutes to download. Please be patient.
<HR WIDTH="100%"></P>

<H1 ALIGN=CENTER>Simulation Definition:</H1>

<P><B>See the source HTML code for this page to see the simulation definition
that is parsed and executed by the <A HREF="../index.html">WebSim</A> applet.<BR>
<BR>
<FONT SIZE=+1>MDP</FONT>:</B> <FONT SIZE=+0>a linear-quadratic regulator.
State space is a section of the number line from [-1,1]. An imaginary cart
sits on this number line and has two actions possible: move left or move
right. The act of moving left corresponds to an input to the neural network
of -1. The act of moving right corresponds to an input to the neural network
of 1. The state is the position on the number line. The cost function is
the position on the number line squared after performing an action. The
goal is to minimize the cost. There is no absorbing state.<BR>
<BR>
</FONT><B><FONT SIZE=+1>Function Approximator</FONT>:</B> <FONT SIZE=+0>a
single-hidden-layer sigmoidal network with 8 nodes in the hidden layer.<BR>
<BR>
</FONT><B><FONT SIZE=+1>Learning algorithm</FONT>:</B> <FONT SIZE=+0>Backprop<BR>
<BR>
</FONT><B><FONT SIZE=+1>RL algorithm</FONT>: </B><FONT SIZE=+0>Residual
Gradient QLearning<BR>
<BR>
</FONT><B><FONT SIZE=+1>Displays</FONT>: <BR>
</B><FONT SIZE=+1>1)</FONT><FONT SIZE=+0> Variables and Rates (upper left
corner)<BR>
</FONT><FONT SIZE=+1>2)</FONT><FONT SIZE=+0> 2D graph of log error vs.
learning time (upper right corner)<BR>
</FONT><FONT SIZE=+1>3)</FONT><FONT SIZE=+0> 3D graph of value function
(lower left corner)<BR>
</FONT><FONT SIZE=+1>4)</FONT><FONT SIZE=+0> 3D graph of policy (lower
right corner)</FONT></P>

The 3D graphs can be rotated on two different axis by clicking
and dragging inside or outside of the box.
</P>

<P><FONT SIZE=+0><B>Value Function Display:</B>&nbsp;After learning, the
value function will look like a &quot;U&quot;. Remember that the value
of a state is the maximum Q-value in the given state. Also, the definition
of a Q-value is the sum of the reinforcements recieved when performing
the corresponding action followed by optimal policy thereafter. The X-axis
corresponds to state space. The Z-axis (height) is the value in each state.
The Y-axis (depth) has no meaning.</FONT></P>

<P><FONT SIZE=+0><B>Policy Display:</B> The policy for this system is clear.
When the &quot;cart&quot; is left of 0, the RL&nbsp;system should perform
action 1. If the &quot;cart&quot; is right of 0 the RL&nbsp;system should
perform action -1. The X-axis corresponds to state space. The Y-axis is
the policy in each state. The Z-axis has no meaning.</FONT></P>

<P><A HREF="Tut.html">Back</A> to Tutorial. </P>

<P><APPLET codebase="../classes" code=WebSim.class width=620 height=320><PARAM name=sourceText value="
`/*****************************************************************/
`embed
`(0,0,524,60)
`Simulator {
`  experiment Backprop {
`    learningRate .1
`    momentum  .9
`    tolerance 1e-8
`    smooth    .999
`    error ReinforcementLearning {
`      mdp             LQR
`      algorithm       QLearning
`      method          residual 0.5
`      dt              0.1
`      gamma           0.9
`      incremental     true
`      trajectories    false
`      exploration     1.0
`      funApp          #DEF FUNCTION {Net {Identity Linear Bipolar(8) Linear Identity}}
`    }
`   }
`   displays {
`      embed
`      (0,0,200,200)
`      title {title 'Learned policy vs. state' display
`      Graph3D {
`                rotateX 10 rotateY -93 rotateZ 0
`                trigger 'time' freq 500
`                inputs  [1 0 0]
`                xElement 1 xSamples 21 xMin -1 xMax 1
`                yElement 2 ySamples  2 yMin -1 yMax 1
`                zElement 1
`                function ValuePolicy {
`                                       mdp LQR
`                                       funApp #USE FUNCTION
`                                     }
`                //plotFloor true
`                plotTop true
`                floorColor [0,0,0]
`      }
`      }
`      embed
`      (0,0,200,200)
`      title {title 'Learned value vs. state' display
`      Graph3D {
`                rotateX 10 rotateY -93 rotateZ 0
`                trigger 'time' freq 500
`                inputs  [1 0 0]
`                xElement 1 xSamples 21 xMin -1 xMax 1
`                yElement 2 ySamples  2 yMin -1 yMax 1
`                zElement 0
`                function ValuePolicy {
`                                       mdp LQR
`                                       funApp #USE FUNCTION
`                                     }
`                plotFloor true   //plotTop true
`                floorColor [0,0,0]
`      }
`      }
`
`     embed
`     (0,0,200,200)
`     title {title 'Log error vs. timestep' display
`     Graph2D {
`       trigger 'time' freq 100
`       plots {
`         plotXY {
`           size 100
`           lineColor [0 .5 0] symbolColor [0 .8 0]
`           trigger 'time' freq 100
`           x       'time'
`           y       'log error'
`         }
`       }
`     }
`     }
`   }
`}
`/*****************************************************************/
">
<HR>This Java applet requires a Java-aware browser such as Netscape 2.0
for Solaris/Win95/WinNT.
<HR></APPLET></P>

</BODY>
</HTML>
*****************/
">
<HR>This Java applet requires a Java-aware browser such as Netscape 2.0
for Solaris/Win95/WinNT.
<HR></APPLET></P>

</BODY>
</HTML>
