<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 3.2//EN">
<!--NewPage-->
<html>
<head>
<!-- Generated by javadoc on Tue Jul 29 22:00:59 GMT+01:00 1997 -->
<title>
  Class sim.errFun.RLErrFun
</title>
</head>
<body>
<a name="_top_"></a>
<pre>
<a href="packages.html">All Packages</a>  <a href="tree.html">Class Hierarchy</a>  <a href="Package-sim.errFun.html">This Package</a>  <a href="sim.errFun.QLearning.html#_top_">Previous</a>  <a href="sim.errFun.ReinforcementLearning.html#_top_">Next</a>  <a href="AllNames.html">Index</a></pre>
<hr>
<h1>
  Class sim.errFun.RLErrFun
</h1>
<pre>
java.lang.Object
   |
   +----<a href="sim.errFun.ErrFun.html#_top_">sim.errFun.ErrFun</a>
           |
           +----sim.errFun.RLErrFun
</pre>
<hr>
<dl>
  <dt> public abstract class <b>RLErrFun</b>
  <dt> extends <a href="sim.errFun.ErrFun.html#_top_">ErrFun</a>
</dl>
All RL objects inherit from this class.  RLErrFun is used to allow RL algorithm objects (i.e. QLearning,
 AdvantageLearning, ValueIteration) to have access to the parameters passed to ReinforcementLearning from the
 html file.
    <p>This code is (c) 1997 Mance E. Harmon
    <<a href=mailto:mharmon@acm.org>mharmon@acm.org</a>>,
    <a href=http://www.cs.cmu.edu/~baird/java>http://www.cs.cmu.edu/~baird/java</a><br>
    The source and object code may be redistributed freely.
    If the code is modified, please state so in the comments.
<p>
<dl>
  <dt> <b>Version:</b>
  <dd> 1.03, 21 July 97
  <dt> <b>Author:</b>
  <dd> Mance E. Harmon
</dl>
<hr>
<a name="index"></a>
<h2>
  <img src="images/variable-index.gif" width=207 height=38 alt="Variable Index">
</h2>
<dl>
  <dt> <img src="images/magenta-ball-small.gif" width=6 height=6 alt=" o ">
	<a href="#action"><b>action</b></a>
  <dd> An action that can be chosen in a given state
  <dt> <img src="images/magenta-ball-small.gif" width=6 height=6 alt=" o ">
	<a href="#dt"><b>dt</b></a>
  <dd> The time step size used in transitioning from state x(t) to x(t+1)
  <dt> <img src="images/magenta-ball-small.gif" width=6 height=6 alt=" o ">
	<a href="#endTrajectory"><b>endTrajectory</b></a>
  <dd> Are we at an absorbing state? Used doing batch training where the length of a trajectory is the size of a batch.
  <dt> <img src="images/magenta-ball-small.gif" width=6 height=6 alt=" o ">
	<a href="#exploration"><b>exploration</b></a>
  <dd> The percentage of time a random action is chosen for training.
  <dt> <img src="images/magenta-ball-small.gif" width=6 height=6 alt=" o ">
	<a href="#gamma"><b>gamma</b></a>
  <dd> The discount factor
  <dt> <img src="images/magenta-ball-small.gif" width=6 height=6 alt=" o ">
	<a href="#incremental"><b>incremental</b></a>
  <dd> The mode of learning: incremental or epoch-wise.
  <dt> <img src="images/magenta-ball-small.gif" width=6 height=6 alt=" o ">
	<a href="#mdp"><b>mdp</b></a>
  <dd> the mdp to control
  <dt> <img src="images/magenta-ball-small.gif" width=6 height=6 alt=" o ">
	<a href="#method"><b>method</b></a>
  <dd> Specifies method (0=residual, 1=resGrad, 2=direct)
  <dt> <img src="images/magenta-ball-small.gif" width=6 height=6 alt=" o ">
	<a href="#methodStr"><b>methodStr</b></a>
  <dd> Specifies method "residual", "resGrad" or "direct"
  <dt> <img src="images/magenta-ball-small.gif" width=6 height=6 alt=" o ">
	<a href="#mu"><b>mu</b></a>
  <dd> The decay factor for the trace of the of resgrad and direct update vectors used to calculate phi.
  <dt> <img src="images/magenta-ball-small.gif" width=6 height=6 alt=" o ">
	<a href="#newState"><b>newState</b></a>
  <dd> The state reached after performing an action
  <dt> <img src="images/magenta-ball-small.gif" width=6 height=6 alt=" o ">
	<a href="#phi"><b>phi</b></a>
  <dd> The weighting factor for the weights of resgrad and direct vectors.
  <dt> <img src="images/magenta-ball-small.gif" width=6 height=6 alt=" o ">
	<a href="#state"><b>state</b></a>
  <dd> The state of the MDP
  <dt> <img src="images/magenta-ball-small.gif" width=6 height=6 alt=" o ">
	<a href="#statesOnly"><b>statesOnly</b></a>
  <dd> Used to tell ReinforcementLearning if this algorithm uses states only (as opposed to state/action pairs).
  <dt> <img src="images/magenta-ball-small.gif" width=6 height=6 alt=" o ">
	<a href="#trajectories"><b>trajectories</b></a>
  <dd> Should we follow trajectories.
  <dt> <img src="images/magenta-ball-small.gif" width=6 height=6 alt=" o ">
	<a href="#valueKnown"><b>valueKnown</b></a>
  <dd> A flag stating whether or not we know for certain the value of a state.
</dl>
<h2>
  <img src="images/constructor-index.gif" width=275 height=38 alt="Constructor Index">
</h2>
<dl>
  <dt> <img src="images/yellow-ball-small.gif" width=6 height=6 alt=" o ">
	<a href="#RLErrFun()"><b>RLErrFun</b></a>()
  <dd> 
</dl>
<h2>
  <img src="images/method-index.gif" width=207 height=38 alt="Method Index">
</h2>
<dl>
  <dt> <img src="images/red-ball-small.gif" width=6 height=6 alt=" o ">
	<a href="#initVects(sim.mdp.MDP, sim.errFun.RLErrFun)"><b>initVects</b></a>(MDP, RLErrFun)
  <dd> Used to initialize the inputs, state, and action vectors in all RL algorithm objects (not ReinforcementLearning).
</dl>
<a name="variables"></a>
<h2>
  <img src="images/variables.gif" width=153 height=38 alt="Variables">
</h2>
<a name="mdp"><img src="images/magenta-ball.gif" width=12 height=12 alt=" o "></a>
<b>mdp</b>
<pre>
 protected <a href="sim.mdp.MDP.html#_top_">MDP</a> mdp
</pre>
<dl>
  <dd> the mdp to control<p>
</dl>
<a name="state"><img src="images/magenta-ball.gif" width=12 height=12 alt=" o "></a>
<b>state</b>
<pre>
 protected <a href="matrix.MatrixD.html#_top_">MatrixD</a> state
</pre>
<dl>
  <dd> The state of the MDP<p>
</dl>
<a name="newState"><img src="images/magenta-ball.gif" width=12 height=12 alt=" o "></a>
<b>newState</b>
<pre>
 protected <a href="matrix.MatrixD.html#_top_">MatrixD</a> newState
</pre>
<dl>
  <dd> The state reached after performing an action<p>
</dl>
<a name="action"><img src="images/magenta-ball.gif" width=12 height=12 alt=" o "></a>
<b>action</b>
<pre>
 protected <a href="matrix.MatrixD.html#_top_">MatrixD</a> action
</pre>
<dl>
  <dd> An action that can be chosen in a given state<p>
</dl>
<a name="dt"><img src="images/magenta-ball.gif" width=12 height=12 alt=" o "></a>
<b>dt</b>
<pre>
 protected NumExp dt
</pre>
<dl>
  <dd> The time step size used in transitioning from state x(t) to x(t+1)<p>
</dl>
<a name="phi"><img src="images/magenta-ball.gif" width=12 height=12 alt=" o "></a>
<b>phi</b>
<pre>
 protected NumExp phi
</pre>
<dl>
  <dd> The weighting factor for the weights of resgrad and direct vectors.<p>
</dl>
<a name="mu"><img src="images/magenta-ball.gif" width=12 height=12 alt=" o "></a>
<b>mu</b>
<pre>
 protected NumExp mu
</pre>
<dl>
  <dd> The decay factor for the trace of the of resgrad and direct update vectors used to calculate phi.<p>
</dl>
<a name="method"><img src="images/magenta-ball.gif" width=12 height=12 alt=" o "></a>
<b>method</b>
<pre>
 protected NumExp method
</pre>
<dl>
  <dd> Specifies method (0=residual, 1=resGrad, 2=direct)<p>
</dl>
<a name="gamma"><img src="images/magenta-ball.gif" width=12 height=12 alt=" o "></a>
<b>gamma</b>
<pre>
 protected NumExp gamma
</pre>
<dl>
  <dd> The discount factor<p>
</dl>
<a name="methodStr"><img src="images/magenta-ball.gif" width=12 height=12 alt=" o "></a>
<b>methodStr</b>
<pre>
 protected <a href="pointer.PString.html#_top_">PString</a> methodStr
</pre>
<dl>
  <dd> Specifies method "residual", "resGrad" or "direct"<p>
</dl>
<a name="incremental"><img src="images/magenta-ball.gif" width=12 height=12 alt=" o "></a>
<b>incremental</b>
<pre>
 protected <a href="pointer.PBoolean.html#_top_">PBoolean</a> incremental
</pre>
<dl>
  <dd> The mode of learning: incremental or epoch-wise.<p>
</dl>
<a name="exploration"><img src="images/magenta-ball.gif" width=12 height=12 alt=" o "></a>
<b>exploration</b>
<pre>
 protected NumExp exploration
</pre>
<dl>
  <dd> The percentage of time a random action is chosen for training.<p>
</dl>
<a name="statesOnly"><img src="images/magenta-ball.gif" width=12 height=12 alt=" o "></a>
<b>statesOnly</b>
<pre>
 protected boolean statesOnly
</pre>
<dl>
  <dd> Used to tell ReinforcementLearning if this algorithm uses states only (as opposed to state/action pairs).
 If the RL algorithm being implemented uses only states, then this variable must be set to true.<p>
</dl>
<a name="endTrajectory"><img src="images/magenta-ball.gif" width=12 height=12 alt=" o "></a>
<b>endTrajectory</b>
<pre>
 protected <a href="pointer.PBoolean.html#_top_">PBoolean</a> endTrajectory
</pre>
<dl>
  <dd> Are we at an absorbing state? Used doing batch training where the length of a trajectory is the size of a batch.<p>
</dl>
<a name="valueKnown"><img src="images/magenta-ball.gif" width=12 height=12 alt=" o "></a>
<b>valueKnown</b>
<pre>
 protected <a href="pointer.PBoolean.html#_top_">PBoolean</a> valueKnown
</pre>
<dl>
  <dd> A flag stating whether or not we know for certain the value of a state.<p>
</dl>
<a name="trajectories"><img src="images/magenta-ball.gif" width=12 height=12 alt=" o "></a>
<b>trajectories</b>
<pre>
 protected <a href="pointer.PBoolean.html#_top_">PBoolean</a> trajectories
</pre>
<dl>
  <dd> Should we follow trajectories.<p>
</dl>
<a name="constructors"></a>
<h2>
  <img src="images/constructors.gif" width=231 height=38 alt="Constructors">
</h2>
<a name="RLErrFun"></a>
<a name="RLErrFun()"><img src="images/yellow-ball.gif" width=12 height=12 alt=" o "></a>
<b>RLErrFun</b>
<pre>
 public RLErrFun()
</pre>
<a name="methods"></a>
<h2>
  <img src="images/methods.gif" width=151 height=38 alt="Methods">
</h2>
<a name="initVects(sim.mdp.MDP, sim.errFun.RLErrFun)"><img src="images/red-ball.gif" width=12 height=12 alt=" o "></a>
<a name="initVects"><b>initVects</b></a>
<pre>
 public abstract void initVects(<a href="sim.mdp.MDP.html#_top_">MDP</a> mpd,
                                <a href="#_top_">RLErrFun</a> rl)
</pre>
<dl>
  <dd> Used to initialize the inputs, state, and action vectors in all RL algorithm objects (not ReinforcementLearning).
<p>
</dl>
<hr>
<pre>
<a href="packages.html">All Packages</a>  <a href="tree.html">Class Hierarchy</a>  <a href="Package-sim.errFun.html">This Package</a>  <a href="sim.errFun.QLearning.html#_top_">Previous</a>  <a href="sim.errFun.ReinforcementLearning.html#_top_">Next</a>  <a href="AllNames.html">Index</a></pre>
</body>
</html>
