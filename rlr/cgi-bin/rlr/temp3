ar
Robot problems represent a class of RL methods which have noisy sensors
and effectors.  

dp

Dynamic programming (DP) reinforcement learning methods are a class of
problems in which optimal policies are computed based on a given Markov
decision process
(MDP) model of the environment.  DP methods use value functions to
structure the search for optimal policies.  Two basic approaches to this
search are policy iteration and value iteration.  A newer variation is
modified policy iteration.  Because DP methods are based
on MDPs, they require a complete and accurate model of the environment.

References:

An extensive discussion of MDPs may be found in:

Puterman, M.  Markov Decision Processes: Discrete Dynamic Stochastic
Programming.  John Wiley, 1994.


fa
To solve problems with continuous or large finite state spaces, it is
crucial to
approximate the value function during reinforcement learning or dynamic
programming. A number
of techniques work well in practice, however, the theoretical properties
of these methods are not yet well understood.  In addition, there are some
situations for which the existing methods are not suited; new algorithms
must be developed (Report of the 1996 Workshop on Reinforcement Learning, 
Mahadevan and Kaelbling).


hm
The motivation for the use of hierarchical models in RL is the goal of
faster learning through decomposition of a task into a collection of
simpler subtasks.   

in
RL has been applied to several problems in industrial process
control.  These include packaging control, optimal scheduling of a
multiple-elevator system, and job-shop scheduling (scheduling of a set of
tasks to satisfy a set of both temporal and resource constraints).

nb


pl
Planning is concerned with

po

The vast majority of RL work concentrates on the completely observable
case, in which there is noise in the agent's actions, but none in its
observation of the environment.  

When the state space is not completely observable, additional methods need
to be used beyond those which apply to MDPs.  Partially observable 
problems typically use probabilistic methods such as Bayesian belief
networks.  

Richer models 

sh
Shaping


td
Temporal difference (TD) learning methods are a class of problems which
learn by iteratively reducing the discrepancies between the estimates
produced
by the agent at different times. TD methods do not require a model.
Q learning is a widely used TD method.

th


un
Average reward and undiscounted methods 
