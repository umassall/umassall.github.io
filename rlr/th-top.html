<html>
<head>
<title>Reinforcement Learning Repository at MSU</title>
<script type="text/javascript">

  var _gaq = _gaq || [];
  _gaq.push(['_setAccount', 'UA-35723838-1']);
  _gaq.push(['_trackPageview']);

  (function() {
    var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
    ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
    var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
  })();

</script>
</head>
<body bgcolor="FFFFBB">
<base target="rl-main">
</head>

<body bgcolor="FFFFBB">
<h3>Reinforcement Learning Repository at MSU</h3>
<img align=left src="imgs/bookmark.gif">
<font size=6><b>Topics: Applications to Theoretical Analysis</font></b>

<br>
<br><br><br>


<p>
A key question regarding reinforcement learning
methods is whether they will converge, in the limit, to the optimal 
value function. Several of the most popular discounted algorithms,  
such as Q-learning and TD(lambda), have convergence proofs, assuming
that the value function is represented in a tabular
fashion. Theoretical analysis of these methods in the general case
where a nonlinear function approximator is used is still an open
problem, as is the case when undiscounted methods are used.

<br><br>
   <a href=th.html>publications</a> &nbsp
