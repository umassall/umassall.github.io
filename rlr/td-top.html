<html>
<head>
<title>Reinforcement Learning Repository at MSU</title>
</head>
<body bgcolor="FFFFBB">
<base target="rl-main">
<script type="text/javascript">

  var _gaq = _gaq || [];
  _gaq.push(['_setAccount', 'UA-35723838-1']);
  _gaq.push(['_trackPageview']);

  (function() {
    var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
    ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
    var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
  })();

</script>
</head>

<body bgcolor="FFFFBB">
<h3>Reinforcement Learning Repository at MSU</h3>
<img align=left src="imgs/bookmark.gif">
<font size=6><b>Topics: Temporal Difference Learning</font></b>

<br>
<br><br><br>


<p>
Temporal difference (TD) methods is a general framework
for solving sequential prediction and control problems, whereby an
agent learns by comparing temporally successive predictions. A key  
strength of TD methods is that the agent can learn before seeing the
final outcome. Q-learning is one of the most popular TD methods.

<br><br>
   <a href=ar.html>publications</a> &nbsp
