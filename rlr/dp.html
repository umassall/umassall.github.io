<HTML>
<HEAD>
<META HTTP-EQUIV=REFRESH>
<TITLE>Publications on Dynamic Programming/Markov Decision
Processes</TITLE>
<script type="text/javascript">

  var _gaq = _gaq || [];
  _gaq.push(['_setAccount', 'UA-35723838-1']);
  _gaq.push(['_trackPageview']);

  (function() {
    var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
    ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
    var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
  })();

</script>
</HEAD>
<body bgcolor="#FFFFBB">
<CENTER>
<H1>Publications on Dynamic Programming/Markov Decision Processes</H1>
</CENTER>

<img align=right src="imgs/sower.gif"> 
<br><hr>

<!nextperson><B>Barto, Andrew</B>
 , S.J. Bradtke and Satinder Singh( <A
	   HREF=mailto:baveja@cs.colorado.edu>baveja@cs.colorado.edu</A>)<BR>
<blockquote><A HREF=ftp://ftp.cs.colorado.edu/users/baveja/Papers/realtime-dp.ps.gz>Learning to Act using Real-Time Dynamic Programming</A><BR>
<i>Learning to Act using Real-Time Dynamic Programming</i>

( gzipped Postscript - 520 KB)
<A HREF=http://web.cps.msu.edu/rlr/pub/Singh2.html>Abstract</A>: 
<BR>Learning methods based on dynamic programming (DP) are receiving
increasing attention in artificial...
</BLOCKQUOTE><HR>

<!nextperson><B>Bhulai, 
Sandjai</B>
<blockquote><B>E-mail:</B> <A
	   HREF=mailto:sbhulai@cs.vu.nl>sbhulai@cs.vu.nl</A><BR>
<A HREF=http://www.cs.vu.nl/~sbhulai/papers/thesis.html>Markov Decision Processes: the control of high-dimensional systems.</A><BR>
<i>Ph.D. Thesis, Vrije Universiteit Amsterdam, 2002</i>

(PDF - 1.1 MB)
<A HREF=http://www-anw.cs.umass.edu/rlr/pub/Bhulai2.html>Abstract</A>: 
<BR>We develop algorithms for the computation of (nearly) optimal decision rules in high-dimensional sys...
</BLOCKQUOTE><HR>
<!nextperson><B>Bhulai, 
Sandjai</B>
( <A
	   HREF=mailto:sbhulai@cs.vu.nl>sbhulai@cs.vu.nl</A>)<BR>
<blockquote><A HREF=http://www.cs.vu.nl/~sbhulai/papers/thesis.html>Markov Decision Processes: the control of high-dimensional systems.</A><BR>
<i>Ph.D. Thesis, Vrije Universiteit Amsterdam, 2002</i>

(PDF - 1.1 MB)
<A HREF=http://www-anw.cs.umass.edu/rlr/pub/Bhulai3.html>Abstract</A>: 
<BR>We develop algorithms for the computation of (nearly) optimal decision rules in high-dimensional sys...
</BLOCKQUOTE><HR>
<!nextperson><B>Diuk, 
Carlos</B>
 , Alexander Strehl, Michael Littman<blockquote><B>E-mail:</B> <A
	   HREF=mailto:cdiuk@cs.rutgers.edu>cdiuk@cs.rutgers.edu</A><BR>
<A HREF=http://paul.rutgers.edu/~cdiuk/papers/HRL_AAMAS.pdf>A Hierarchical Approach to Efficient Reinforcement Learning in Deterministic Domains</A><BR>
<i>AAMAS 2006</i>

(PDF - 140KB)
<A HREF=http://www-anw.cs.umass.edu/rlr/pub/Diuk1.html>Abstract</A>: 
<BR>Factored representations, model-based learning, and hierar-
chies are well-studied techniques for i...
</BLOCKQUOTE><HR>
<!nextperson><B>Ernst, 
Damien</B>
 , Pierre Geurts and Louis Wehenkel<blockquote><B>E-mail:</B> <A
	   HREF=mailto:dernst@ulg.ac.be>dernst@ulg.ac.be</A><BR>
<A HREF=http://www.montefiore.ulg.ac.be/~ernst/>Iteratively extending time horizon reinforcement learning</A><BR>
<i>Proceedings of ECML 2003</i>

(Postscript - 6 KB)
<A HREF=http://www-anw.cs.umass.edu/rlr/pub/Ernst2.html>Abstract</A>: 
<BR>Reinforcement learning  aims to  determine an (infinite  time horizon)
optimal  control policy  fro...
</BLOCKQUOTE><HR>
<!nextperson><B>Gabor, 
Zoltan</B>
 , Zs. Kalmar and Cs. Szepesvari ( <A
	   HREF=mailto:szepes@mindmaker.kfkipark.hu>szepes@mindmaker.kfkipark.hu</A>)<BR>
<blockquote><A HREF=http://rgai00.inf.u-szeged.hu/rgai/techrep/115.ps.gz>Multi-criteria Reinforcement Learning </A><BR>
<i>Technical Report TR-98-115, "Attila József" University, Research Group on Artificial Intelligence Szeged, HU-6700, 1998 </i>

( gzipped Postscript - 153 Kb)
<A HREF=http://web.cps.msu.edu/rlr/pub/Gabor1.html>Abstract</A>: 
<BR>This is a longer version of the paper published in ICML'98.

We consider multi-criteria sequential...
</BLOCKQUOTE><HR>
<!nextperson><B>Goldsmith, 
Judy </B>
<blockquote><B>E-mail:</B> <A
	   HREF=mailto:goldsmit at cs.uky.edu>goldsmit at cs.uky.edu</A><BR>
<A HREF=http://www.cs.uky.edu/~goldsmit/papers/papers.html>Papers</A><BR>
<i>various</i>

<A HREF=http://www-anw.cs.umass.edu/rlr/pub/Goldsmith2.html>Abstract</A>: 
<BR>...
</BLOCKQUOTE><HR>
<!nextperson><B>Guestrin, 
Carlos</B>
 , Daphne Koller, Ronald Parr<blockquote><B>E-mail:</B> <A
	   HREF=mailto:guestrin@cs.stanford.edu>guestrin@cs.stanford.edu</A><BR>
<A HREF=http://robotics.stanford.edu/~koller/papers/ijcai01gkp.html>Max-norm Projections for Factored MDPs</A><BR>
<i>AAAI Spring Symposium, Stanford, California, March 2001</i>

(Postscript - 323KB)
<A HREF=http://www.cse.msu.edu/rlr/pub/Guestrin1.html>Abstract</A>: 
<BR>Markov Decision Processes (MDPs) provide a coherent mathematical framework for planning under uncert...
</BLOCKQUOTE><HR>
<!nextperson><B>Littman, Michael</B>, Thomas L. Dean and Leslie Pack 
Kaelbling<blockquote>
<A HREF= http://www.cs.duke.edu/~mlittman/docs/mdp-complexity.ps>
On the complexity of solving Markov decision problems</A><BR>
 
<I> Proceedings of the Eleventh Annual Conference
 on Uncertainty in Artificial Intelligence (UAI--95) </I>
 
(Postscript - 256KB)
 
<br><a href=http://www.cps.msu.edu/rlr/pub/Littman2.html>
Abstract</a>: Markov decision problems (MDPs) provide the 
foundations for a number
of problems of interest to AI researchers studying automated planning
and reinforcement learning.  In this paper, we summarize results
regarding the complexity of solving MDPs and the running time of MDP...
</BLOCKQUOTE>
<hr>

<!nextperson><B>Munos, 
Remi</B>
 , Andrew Moore<blockquote><B>E-mail:</B> <A
	   HREF=mailto:munos@cs.cmu.edu>munos@cs.cmu.edu</A><BR>
<A HREF=http://www.cs.cmu.edu/~munos/papers/ijcai99.ps.gz>Variable resolution discretization for high-accuracy solutions of 
optimal control problems</A><BR>
<i>IJCAI'99</i>

( gzipped Postscript - 315KB)
<A HREF=http://www.cse.msu.edu/rlr/pub/Munos6.html>Abstract</A>: 
<BR>State abstraction is of central importance in reinforcement learning and Markov Decision Processes. ...
</BLOCKQUOTE><HR>
<!nextperson><B>Ormoneit, 
Dirk</B>
 , Saunak Sen<blockquote><B>E-mail:</B>
<A HREF=mailto:ormoneit@stat.stanford.edu>ormoneit@stat.stanford.edu</A><BR>
<A HREF=http://www.robotics.stanford.edu/~ormoneit/publications/tr-1999-8-updated.ps>Kernel-Based Reinforcement Learning</A><BR>
<i>Department of Statistics, Stanford University, Technical Report No. 1999-8</i>

(Postscript - 260 KB)
<A HREF=http://www.cse.msu.edu/rlr/pub/Ormoneit1.html>Abstract</A>: 
<BR>Kernel-based methods have recently attracted increased attention in
the machine learning literature...
</BLOCKQUOTE><HR>
<!nextperson><B>Pouget, 
A.</B>
 , Deffayet, C. and Sejnowski, T. J.<blockquote><i>In: G. Tesauro, D. Touretzky and J. Alspector (Eds.) Advances in Neural Information Processing Systems 7,
 MIT Press, Cambridge, MA, 125-132 (1995).</i>

<A HREF=http://web.cps.msu.edu/rlr/pub/Pouget1.html>Abstract</A>: 
<BR>(no abstract available)...
</BLOCKQUOTE><HR>
<!nextperson><B>Schmidhuber, 
Jurgen</B>
 , M. Wiering<blockquote><B>E-mail:</B> <A
	   HREF=mailto:juergen@isdia.ch>juergen@isdia.ch</A><BR>
<a href=ftp://ftp.idsia.ch/pub/juergen/hq96.ps.gz>HQ-Learning: Discovering
Markovian subgoals for non-Markovian reinforcement learning</a><br>
<i>Technical Report IDSIA-95-96, October 1996</i>

( gzipped Postscript - 111 KB)
<A HREF=http://web.cps.msu.edu/rlr/pub/Schmidhuber1.html>Abstract</A>: 
<BR>To solve partially observable Markov decision problems, we introduce
HQ-learning, a hierarchical ex...
</BLOCKQUOTE><HR>
<!nextperson><B>Singh, 
Satinder</B>
( <A
	   HREF=mailto:bajeva@cs.colorado.edu>bajeva@cs.colorado.edu</A>)<BR>
<blockquote><A HREF=ftp://ftp.cs.colorado.edu/users/baveja/Papers/Thesis.ps.gz>Learning to Solve Markov Decision Processes</A><BR>
<i>Ph.D. thesis, University of Massachusetts, Amherst, 1994 </i>

( gzipped Postscript - 676 KB)
<A HREF=http://web.cps.msu.edu/rlr/pub/Singh1.html>Abstract</A>: 
<BR>ABSTRACT: This dissertation is about building learning control architectures for
       agents embe...
</BLOCKQUOTE><HR>
<!nextperson><B>Strens, 
Malcolm</B>
( <A
	   HREF=mailto:mjstrens@qinetiq.com>mjstrens@qinetiq.com</A>)<BR>
<blockquote><A HREF=http://uk.geocities.com/mjstrens/icml.pdf>A Bayesian Framework for Reinforcement Learning</A><BR>
<i>International Conference on Machine Learning, 2000</i>

(pdf - 83KB)
<A HREF=http://www-anw.cs.umass.edu/rlr/pub/Strens1.html>Abstract</A>: 
<BR>The reinforcement learning problem can be decomposed into two parallel types of inference: (i) estim...
</BLOCKQUOTE><HR>
<!nextperson><B>Sutton, 
Richard</B>
( <A
	   HREF=mailto:rich@cs.umass.edu>rich@cs.umass.edu</A>)<BR>
<blockquote><A HREF=ftp://ftp.cs.umass.edu/pub/anw/pub/sutton/sutton-91a.ps.gz>Planning by incremental dynamic programming</A><BR>
<i></i>

( gzipped Postscript - 55 KB)
<A HREF=http://web.cps.msu.edu/rlr/pub/Sutton2.html>Abstract</A>: 
<BR>This paper presents the basic results and ideas of dynamic
       programming as they relate most d...
</BLOCKQUOTE><HR>
<!nextperson><B>Szepesvari, 
Csaba</B>
( <A
	   HREF=mailto:szepes@mindmaker.kfkipark.hu>szepes@mindmaker.kfkipark.hu</A>)<BR>
<blockquote><A HREF=http://ultralix.polytechnique.fr/~weinfeld/ICANN/>General Framework for Reinforcement Learning </A><BR>
<i>Proceedings of ICANN'95 Paris, France, Oct. 1995, Vol. II., pp. 165-170 </i>

( gzipped Postscript - ??)
<A HREF=http://web.cps.msu.edu/rlr/pub/Szepesvari4.html>Abstract</A>: 
<BR>In this article we propose a general framework for sequential decision making. The framework is base...
</BLOCKQUOTE><HR>
<!nextperson><B>Szepesvari, 
Csaba</B>
( <A
	   HREF=mailto:szepes@mindmaker.kfkipark.hu>szepes@mindmaker.kfkipark.hu</A>)<BR>
<blockquote><A
HREF=http://iserv.iki.kfki.hu/pub/papers/icnn94/szepes.dcmopt.ps.Z></A>
<b>Dynamic Concept Model Learns Optimal Policies </b>  
<BR>
<i>Proceedings of IEEE WCCI ICNN'94 Vol. III. pp. 1738-1742. Orlando, Florida, June 1994</i>

( gzipped Postscript - ??)
<A HREF=http://web.cps.msu.edu/rlr/pub/Szepesvari5.html>Abstract</A>: 
<BR>Reinforcement learning is a flourishing field of neural methods. It has a firm theoretical basis and...
</BLOCKQUOTE><HR>
<!nextperson><B>Szepesvári, 
Csaba</B>
( <A
	   HREF=mailto:szepes@mindmaker.kfkipark.hu>szepes@mindmaker.kfkipark.hu</A>)<BR>
<blockquote><A
HREF=http://www.inf.u-szeged.hu/local/acta/vol13n3/vol13n3.html>Non-Markovian Policies in Sequential Decision Problems 
Non-Markovian Policies in Sequential Decision Problems 
</A><BR>
<i>Acta Cybernetica, to appear (1998)</i>

( gzipped Postscript - )
<A HREF=http://web.cps.msu.edu/rlr/pub/Szepesvári1.html>Abstract</A>: 
<BR>In this article we prove the validity of the Bellman Optimality Equation and related results for seq...
</BLOCKQUOTE><HR>
<!nextperson><B>Yin, 
ChangMing</B>
<blockquote><B>E-mail:</B> <A
	   HREF=mailto:cmyin@cs167.net>cmyin@cs167.net</A><BR>
<A HREF=http://www.csuep.edu.cn/cs/~cmyin></A>
<b>Forgetting Algorithm for Q-learning</b>
<BR>
<i>unpublished</i>

(Microsoft Word - 120kb)
<A HREF=http://www-anw.cs.umass.edu/rlr/pub/Yin3.html>Abstract</A>: 
<BR>...
</BLOCKQUOTE><HR>
<!nextperson><B>Gabor, 
Zoltan</B>, Zs. Kalmár and Cs. Szepesvári ( <A
	   HREF=mailto:szepes@mindmaker.kfkipark.hu>szepes@mindmaker.kfkipark.hu</A>)<BR>
<blockquote><A HREF=http://www.cs.wisc.edu/icml98/>
Multi-criteria Reinforcement Learning</A>  
<BR>
<i>Proceedings of International Conference of Machine Learning, 1998</i>

( gzipped Postscript - 103 Kb)
<A HREF=http://web.cps.msu.edu/rlr/pub/gabor1.html>Abstract</A>: 
<BR>We consider multi-criteria sequential decision making problems where the vector-valued evaluations a...
</BLOCKQUOTE><HR>
<!nextperson><B>ZHAO, 
Gang</B>
 , Shoji TATSUMI,Ruoying SUN<blockquote><B>E-mail:</B> <A
	   HREF=mailto:zhaogang@ieee.org>zhaogang@ieee.org</A><BR>
<A HREF=http://search.ieice.org/1999/files/e000a10.htm#e82-a,10,2266>RTP-Q: A Reinforcement Learning System with Time Constraints Exploration Planning for Accelerating the Learning Rate</A><BR>
<i>IEICE TRANSACTIONS on Fundamentals of Electronics, Communications and Computer Sciences</i>

(pdf - 171kb)
<A HREF=http://www-anw.cs.umass.edu/rlr/pub/ZHAO1.html>Abstract</A>: 
<BR>This paper proposes a RTP-Q reinforcement learning system which varies an efficient method for explo...
</BLOCKQUOTE><HR>
<!nextperson><B>ZHAO, 
Gang</B>
 , Shoji TATSUMI,Ruoying SUN<blockquote><B>E-mail:</B> <A
	   HREF=mailto:zhaogang@ieee.org>zhaogang@ieee.org</A><BR>
<A HREF=http://search.ieice.org/2000/files/e000a09.htm#e83-a,9,1786>Convergence of the Q-ae Learning on Deterministic MDPs and Its Efficiency on the Stochastic Environment</A><BR>
<i>IEICE TRANSACTIONS on Fundamentals of Electronics, Communications and Computer Sciences</i>

(pdf - 172kb)
<A HREF=http://www-anw.cs.umass.edu/rlr/pub/ZHAO2.html>Abstract</A>: 
<BR>In this paper, based on discussing different exploration methods, replacing the pre-action-selector ...
</BLOCKQUOTE><HR>
