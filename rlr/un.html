<HTML>
<HEAD>
<META HTTP-EQUIV=REFRESH>
<TITLE>Publications on Average-reward/Undiscounted methods</TITLE>
<script type="text/javascript">

  var _gaq = _gaq || [];
  _gaq.push(['_setAccount', 'UA-35723838-1']);
  _gaq.push(['_trackPageview']);

  (function() {
    var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
    ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
    var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
  })();

</script>
</HEAD>
<body bgcolor="#FFFFBB">
<CENTER>
<H1>Publications on Average-reward/Undiscounted methods</H1>
</CENTER>

<img align=right src="imgs/sower.gif">

<br><hr>

<!nextperson><B>Arruda, Edilson F.</B>,  Fragoso, Marcelo D. (<A
           HREF=mailto:efarruda@gmail.com>efarruda@gmail.com</A>)<BR>
<blockquote><A
HREF=http://dx.doi.org/10.1016/j.orl.2011.03.006>Time Aggregated Markov
Decision Processes via Standard Dynamic Programming</A>
<BR>
<i>Operations Research Letters </i> (2011) Abstract:
<BR>This note addresses the time aggregation approach to ergodic finite
state Markov decision processes with uncontrollable states. We propose
the use of the time aggregation approach as an intermediate step toward
constructing a transformed MDP whose state space is comprised solely of
the controllable states. The proposed approach simplifies the iterative
search for the optimal solution by eliminating the need to define an
equivalent parametric function, and results in a problem that can be
solved by simpler, standard MDP algorithms.
</BLOCKQUOTE><HR>

<!nextperson><B>Beleznay, 
Ferenc</B>
 , Tamas Grobler, Csaba Szepesvari( <A
	   HREF=mailto:beleznay@cs.elte.hu>beleznay@cs.elte.hu</A>)<BR>
<blockquote><A
HREF=http://victoria.mindmaker.hu/~szepes/papers/slowql-tr99-02.ps.gz></A>
<b>Comparing Value-Function Estimation Algorithms in Undiscounted Problems</b>
<BR>
<i>unpublished</i>

( gzipped Postscript - 104)
<A HREF=http://www.cse.msu.edu/rlr/pub/Beleznay1.html>Abstract</A>: 
<BR>We compare scaling properties of several value-function estimation algorithms.
In particular, we pr...
</BLOCKQUOTE><HR>
<!nextperson><B>Boutilier, 
Craig</B>
 , Martin L. Puterman ( <A
	   HREF=mailto:cebly@cs.ubc.co>cebly@cs.ubc.co</A>)<BR>
<blockquote><A
HREF=http://www.cs.toronto.edu/~cebly/Papers/_download_/avg-reward.ps.gz>Process-Oriented
Planning and Average-Reward Optimality </A><BR> <i>IJCAI-95 </i>

( gzipped Postscript - 47 KB)
<A HREF=http://web.cps.msu.edu/rlr/pub/Boutilier3.html>Abstract</A>: 
<BR>We argue that many AI planning problems should be viewed as 
<i>process-oriented</i>, where the aim...
</BLOCKQUOTE><HR>
<!nextperson><B>Garcia, 
Frédérick</B>
 , Seydina Ndiaye( <A
	   HREF=mailto:fgarcia@toulouse.inra.fr>fgarcia@toulouse.inra.fr</A>)<BR>
<blockquote><A HREF=http://www-bia.inra.fr/T/garcia/Doc/Papiers/icml98.ps.gz>A Learning Rate Analysis of Reinforcement Learning Algorithms in Fine-Horizon</A><BR>
<i>ICML'98</i>

( gzipped Postscript - 96 KB)
<A HREF=http://www.cse.msu.edu/rlr/pub/Garcia2.html>Abstract</A>: 
<BR>In this article we consider the particular framework of non-stationary 
finite-horizon Markov Decis...
</BLOCKQUOTE><HR>
<!nextperson><B>Mahadevan, 
Sridhar</B>
( <A
	   HREF=mailto:mahadeva@cps.msu.edu>mahadeva@cps.msu.edu</A>)<BR>
<blockquote><A HREF=http://www.cps.msu.edu/~mahadeva/papers/mlj-rl-si-paper.ps.Z>Average Reward Reinforcement Learning:
Foundations, Algorithms, and Empirical Results</A><BR>
<i>Machine Learning ,
Special Issue on Reinforcement Learning (edited by Leslie Kaebling), vol.
22, pp. 159-196, 1996.</i>

(compressed Postscript - )
<A HREF=http://web.cps.msu.edu/rlr/pub/Mahadevan2.html>Abstract</A>: 
<BR>This paper presents a detailed study of average reward reinforcement
learning, an undiscounted opti...
</BLOCKQUOTE><HR>
<!nextperson><B>Ok, 
DoKyeong</B>
 , Prasad Tadepalli( <A
	   HREF=mailto:tadepall@cs.orst.edu>tadepall@cs.orst.edu</A>)<BR>
<blockquote><A
HREF=http://www.cs.orst.edu/~tadepall/research/papers/auto-exploratory.ps>Auto-exploratory average reward reinforcement learning</A><BR>
<i>Proceedings of AAAI-96</i>

(Postscript - 130 KB)
<A HREF=http://web.cps.msu.edu/rlr/pub/Ok1.html>Abstract</A>: 
<BR>We introduce a model-based average-reward Reinforcement Learning
method called H-learning and compa...
</BLOCKQUOTE><HR>
<!nextperson><B>Singh, 
Satinder</B>
( <A
	   HREF=mailto:baveja@cs.colorado.edu>baveja@cs.colorado.edu</A>)<BR>
<blockquote><A HREF=
ftp://ftp.cs.colorado.edu/users/baveja/Papers/AAAI94.ps.gz>
Reinforcement Learning Algorithms for Average-Payoff Markovian
Decision Processes</A><BR>
<i>Proceedings of the Twelth National Conference on Artificial 
Intelligence</i>

( gzipped Postscript - 85 KB)
<A HREF=http://web.cps.msu.edu/rlr/pub/Singh4.html>Abstract</A>: 
<BR>Reinforcement learning (RL) has become a central paradigm for
solving learning-control problems in ...
</BLOCKQUOTE><HR>
<!nextperson><B>Tadepalli, 
Prasad</B>
 , DoKyeong Ok<blockquote><B>E-mail:</B> <A
	   HREF=mailto:tadepall@cs.orst.edu>tadepall@cs.orst.edu</A><BR>
<A HREF=http://www.cs.orst.edu/~tadepall/research/papers/local-linear-regression.ps>Scaling up average reward reinforcement learning by approximating
the domain models and the value function</A><BR>
<i>Proceedings of the Thirteenth International Conference on Machine Learning, pages 471-479.  Morgan Kaufmann, 1996</i>

(Postscript - )
<A HREF=http://web.cps.msu.edu/rlr/pub/Tadepalli1.html>Abstract</A>: 
<BR>Almost all the work in Average-reward Reinforcement Learning (ARL) so
far has focused on table-base...
</BLOCKQUOTE><HR>
<!nextperson><B>Tadepalli, 
Prasad</B>
 , DoKyeong Ok<blockquote><B>E-mail:</B> <A
	   HREF=mailto:tadepall@cs.orst.edu>tadepall@cs.orst.edu</A><BR>
<A HREF=http://www.cs.orst.edu/~tadepall/research/papers/arl-aij.ps>Model-based Average Reward Reinforcement Learning
</A><BR>
<i>Artificial Intelligenec</i>

(Postscript - 53 pages)
<A HREF=http://web.cps.msu.edu/rlr/pub/Tadepalli3.html>Abstract</A>: 
<BR>Reinforcement Learning (RL) is the study of programs that improve their
performance by receiving re...
</BLOCKQUOTE><HR>
