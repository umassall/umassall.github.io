<html>
<head>
<title>Reinforcement Learning Repository at MSU</title>
<script type="text/javascript">

  var _gaq = _gaq || [];
  _gaq.push(['_setAccount', 'UA-35723838-1']);
  _gaq.push(['_trackPageview']);

  (function() {
    var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
    ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
    var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
  })();

</script>
</head>
<body bgcolor="FFFFBB">
<base target="rl-main">
</head>

<body bgcolor="FFFFBB">
<h3>Reinforcement Learning Repository at MSU</h3>
<img align=left src="imgs/bookmark.gif">
<font size=6><b>Topics: Applications to DP/MDP</font></b>

<br>
<br><br><br>


<p>
Dynamic programming (DP) refers to a solution method for
finding optimal solutions to problems with a compositional cost   
structure. Richard Bellman, Ronald Howard, and David Blackwell laid
the foundations for most of the early research into this problem. In
particular, they originated the most popular algorithms (value
iteration and policy iteration), as well as made significant
contributions to the the mathematical study of Markov decision
processes (MDP). Much of the current research into RL is based on the
framework of DP and MDP.  

<br>
<br>
<b>References:</b> An extensive discussion of MDPs may be found in:

<br>Puterman, M.  <i>Markov Decision Processes: Discrete Dynamic Stochastic
Programming.</i>  John Wiley, 1994.



<br><br>
   <a href=dp.html>publications</a> &nbsp
