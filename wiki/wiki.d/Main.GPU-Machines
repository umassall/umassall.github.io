version=pmwiki-2.1.26 ordered=1 urlencoded=1
agent=Mozilla/5.0 (compatible; MSIE 10.0; Windows NT 6.1; WOW64; Trident/6.0)
author=
csum=
ctime=1448922869
host=74.68.98.184
name=Main.GPU-Machines
rev=1
targets=
text=Currently (Nov 2015) we have three GPU machines in our lab: {DIDO,KINGKONG,MANIFOLD}.cs.umass.edu.%0a%0aThe specs (roughly):%0a%0a'''DIDO'''%0a* 2 x Intel(R) Xeon(R) CPU E5-2620 v3 @ 2.40GHz, totalling 12 Cores%0a* 50 GB Memory%0a* 1 x Tesla K40c with 12GB Memory%0a* CUDA 6.5 (needs to be updated)%0a%0a'''KINGKONG'''%0a* 28 Cores @ 2.1GHz%0a* 66 GB Memory%0a* 1 x Tesla K80, with 24GB Memory, spliced into 2 virtual GPUs%0a* CUDA 7.5%0a%0a'''MANIFOLD'''%0a* 2 x Intel(R) Core(TM) i7-5960X CPU @ 3.00GHz, totalling 16 Cores%0a* 66 GB Memory%0a* 2 x GeForce GTX 980 Ti, 6GB Memory each%0a* CUDA 7.5%0a%0a'''Setting up your environment'''%0a%0aStandard deep learning packages are installed on all of the machines (Theano, caffe, Torch etc.). However, to get them running, you need to include the following sys var exports in your ~/.bashrc:%0a    # some environment variables%0a    export OMP_NUM_THREADS=12%0a    export PATH=/usr/local/cuda/bin:/opt/Arcade-Learning-Environment:$PATH%0a    export LD_LIBRARY_PATH=/usr/local/lib:/usr/local/cuda/include:/usr/local/cuda/lib64:$LD_LIBRARY_PATH%0a    export LD_RUN_PATH=/usr/local/lib:$LD_RUN_PATH%0a    export CUDA_ROOT=/usr/local/cuda/bin:%0a    export THEANO_FLAGS='allow_gc=False,cuda.include=/usr/local/cuda/include,cuda.root=/usr/local/cuda/bin,floatX=float32,device=gpu,nvcc.fastmath=True,optimizer_including=conv_meta,metaopt.verbose=1'%0a    export JAVA_HOME='/usr/lib/jvm/java-6-openjdk-amd64/'%0a    export PYLEARN2_DATA_PATH=/nfs/nemo/u3/idurugkar/Documents/code/%0a    export PYTHONPATH=$PYTHONPATH:~/.local/lib/python2.7/site-packages:/nfs/nemo/u3/idurugkar/Downloads/pylearn2:/nfs/nemo/u5/dernbach/python/ale_python_interface:%0a    export PKG_CONFIG_PATH=$PKG_CONFIG_PATH:/usr/local/lib/pkgconfig%0a%0aAt the moment, these sysvars include references to some user's personal home dirs. This should be changed in the near future.%0a%0aOnce these exports are in place, you can check that you can run deep learning packages by doing the following:%0a# run @@nvidia-smi@@ If you can see something other than "Not supported" in the bottom list, the test passes.%0a# run either @@python@@ or @@ipython@@ in the console, and @@import theano@@ ... if you see "Using gpu device x", you're all ready to go.
time=1448922869
author:1448922869=
diff:1448922869:1448922869:=1,42d0%0a%3c Currently (Nov 2015) we have three GPU machines in our lab: {DIDO,KINGKONG,MANIFOLD}.cs.umass.edu.%0a%3c %0a%3c The specs (roughly):%0a%3c %0a%3c '''DIDO'''%0a%3c * 2 x Intel(R) Xeon(R) CPU E5-2620 v3 @ 2.40GHz, totalling 12 Cores%0a%3c * 50 GB Memory%0a%3c * 1 x Tesla K40c with 12GB Memory%0a%3c * CUDA 6.5 (needs to be updated)%0a%3c %0a%3c '''KINGKONG'''%0a%3c * 28 Cores @ 2.1GHz%0a%3c * 66 GB Memory%0a%3c * 1 x Tesla K80, with 24GB Memory, spliced into 2 virtual GPUs%0a%3c * CUDA 7.5%0a%3c %0a%3c '''MANIFOLD'''%0a%3c * 2 x Intel(R) Core(TM) i7-5960X CPU @ 3.00GHz, totalling 16 Cores%0a%3c * 66 GB Memory%0a%3c * 2 x GeForce GTX 980 Ti, 6GB Memory each%0a%3c * CUDA 7.5%0a%3c %0a%3c '''Setting up your environment'''%0a%3c %0a%3c Standard deep learning packages are installed on all of the machines (Theano, caffe, Torch etc.). However, to get them running, you need to include the following sys var exports in your ~/.bashrc:%0a%3c     # some environment variables%0a%3c     export OMP_NUM_THREADS=12%0a%3c     export PATH=/usr/local/cuda/bin:/opt/Arcade-Learning-Environment:$PATH%0a%3c     export LD_LIBRARY_PATH=/usr/local/lib:/usr/local/cuda/include:/usr/local/cuda/lib64:$LD_LIBRARY_PATH%0a%3c     export LD_RUN_PATH=/usr/local/lib:$LD_RUN_PATH%0a%3c     export CUDA_ROOT=/usr/local/cuda/bin:%0a%3c     export THEANO_FLAGS='allow_gc=False,cuda.include=/usr/local/cuda/include,cuda.root=/usr/local/cuda/bin,floatX=float32,device=gpu,nvcc.fastmath=True,optimizer_including=conv_meta,metaopt.verbose=1'%0a%3c     export JAVA_HOME='/usr/lib/jvm/java-6-openjdk-amd64/'%0a%3c     export PYLEARN2_DATA_PATH=/nfs/nemo/u3/idurugkar/Documents/code/%0a%3c     export PYTHONPATH=$PYTHONPATH:~/.local/lib/python2.7/site-packages:/nfs/nemo/u3/idurugkar/Downloads/pylearn2:/nfs/nemo/u5/dernbach/python/ale_python_interface:%0a%3c     export PKG_CONFIG_PATH=$PKG_CONFIG_PATH:/usr/local/lib/pkgconfig%0a%3c %0a%3c At the moment, these sysvars include references to some user's personal home dirs. This should be changed in the near future.%0a%3c %0a%3c Once these exports are in place, you can check that you can run deep learning packages by doing the following:%0a%3c # run @@nvidia-smi@@ If you can see something other than "Not supported" in the bottom list, the test passes.%0a%3c # run either @@python@@ or @@ipython@@ in the console, and @@import theano@@ ... if you see "Using gpu device x", you're all ready to go.%0a\ No newline at end of file%0a
host:1448922869=74.68.98.184
